<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Personal blog of John Novak</title>
        <description>Personal blog of John Novak</description>
        <link>http://blog.johnnovak.net</link>
        <atom:link href="http://blog.johnnovak.net/feed.xml" rel="self" type="application/rss+xml" />
        
            <item>
                <title>What every coder should know about gamma</title>
                <description>&lt;h2 id=&quot;a-short-quiz&quot;&gt;A short quiz&lt;/h2&gt;

&lt;p&gt;If you have ever written, or are planning to write, &lt;em&gt;any kind of code&lt;/em&gt; that
deals with image processing, you should complete the below quiz. If you have answered
one or more questions with a yes, there’s a high chance that your code is
doing the wrong thing and will produce incorrect results. This might not be
immediately obvious to you because these issues can be subtle and they’re
easier to spot in some problem domains than in others.&lt;/p&gt;

&lt;p&gt;So here’s the quiz:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I don’t know what gamma correction is (duh!)&lt;/li&gt;
  &lt;li&gt;Gamma is a relic from the CRT display era; now that almost everyone uses
LCDs, it’s safe to ignore it.&lt;/li&gt;
  &lt;li&gt;Gamma is only relevant for graphics professionals working in the print
industry where accurate colour reproduction is of great
importance—for general image processing, it’s safe to ignore it.&lt;/li&gt;
  &lt;li&gt;I’m a game developer, I don’t need to know about gamma.&lt;/li&gt;
  &lt;li&gt;The graphics libraries of my operating system handle gamma correctly.&lt;sup id=&quot;fnref:osgamma&quot;&gt;&lt;a href=&quot;#fn:osgamma&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;The popular graphics library &lt;em&gt;&amp;lt;insert name here&amp;gt;&lt;/em&gt; I’m using handles gamma correctly.&lt;/li&gt;
  &lt;li&gt;Pixels with RGB values of (128, 128, 128) emit about half as much light as
pixels with RGB values of (255, 255, 255).&lt;/li&gt;
  &lt;li&gt;It is okay to just load pixel data from a popular image format (JPEG, PNG,
GIF etc.) into a buffer using some random library and run image processing
algorithms on the raw data directly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Don’t feel bad if you have answered most with a yes! I would have given
a yes to most of these questions a week ago myself too.  Somehow, the topic
of gamma is just under most computer users’ radar (including programmers
writing commercial graphics software!), to the extent that most graphics
libraries, image viewers, photo editors and drawing software of today still
don’t get gamma right and produce incorrect results.&lt;/p&gt;

&lt;p&gt;So keep on reading, and by the end of this article you’ll be more
knowledgeable about gamma than the vast majority of programmers!&lt;/p&gt;

&lt;h2 id=&quot;the-arcane-art-of-gamma-correctness&quot;&gt;The arcane art of gamma-correctness&lt;/h2&gt;

&lt;p&gt;Given that vision is arguably the most important sensory input channel for
human-computer interaction, it is quite surprising that gamma correction is
one of the least talked about subjects among programmers and it’s mentioned in
technical literature rather infrequently, &lt;em&gt;including&lt;/em&gt; computer graphics texts.
The fact that most computer graphics textbooks don’t explicitly mention the
importance of correct gamma handling, or discuss it in practical terms, does
not help matters at all (my &lt;a href=&quot;http://sirkan.iit.bme.hu/~szirmay/szamgraf.html&quot;&gt;CG textbook from
uni&lt;/a&gt; falls squarely into this
category, I’ve just checked). Some books mention gamma correction in passing
in somewhat vague and abstract terms, but then provide neither concrete
real-world examples on how to do it properly, nor explain what the
implications of not doing it properly are, nor show image examples of
incorrect gamma handling.&lt;/p&gt;

&lt;p&gt;I came across the need for correct gamma handling during writing my &lt;a href=&quot;/tag/ray%20tracing/&quot;&gt;ray
tracer&lt;/a&gt; and I had to admit that my understanding of the
topic was rather superficial and incomplete. So I had spent a few days reading
up on it online, but it turned out that many articles about gamma are not much
help either, as many of them are too abstract and confusing, some contain too
many interesting but otherwise irrelevant details, and then some others lack
image examples or are just simply incorrect or hard to understand. Gamma is
not a terribly difficult concept to begin with, but for some mysterious reason
it’s not that trivial to find articles on it that are correct, complete and
explain the topic in a clear language.&lt;/p&gt;

&lt;h2 id=&quot;what-is-gamma-and-why-do-we-need-it&quot;&gt;What is gamma and why do we need it?&lt;/h2&gt;

&lt;p&gt;Alright, so this is my attempt to offer a comprehensive explanation of gamma,
focusing just on the most important aspects and assuming no prior knowledge of
it.&lt;/p&gt;

&lt;p&gt;The image examples in this article assume that you are viewing this web page in
a modern browser on a computer monitor (CRT or LCD, doesn’t matter). Tablets
and phones are generally quite inaccurate compared to monitors, so try to
avoid those. You should be viewing the images in a dimly lit room, so no
direct lights or flare on your screen please.&lt;/p&gt;

&lt;h3 id=&quot;light-emission-vs-perceptual-brightness&quot;&gt;Light emission vs perceptual brightness&lt;/h3&gt;

&lt;p&gt;Believe it or not, the difference of &lt;strong&gt;light energy emission&lt;/strong&gt; between any two
neighbouring vertical bars in the below image is a &lt;em&gt;constant&lt;/em&gt;. In other words,
the amount of light energy emitted by your screen increases by a &lt;em&gt;constant
amount&lt;/em&gt; from bar to bar, left to right.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/linear-ramp32.png&quot; alt=&quot;Figure 1 &amp;mdash; Evenly-spaced greyscale bars in terms of emitted light intensity&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%; text-align: center;&quot;&gt;Figure 1 &amp;mdash; Evenly-spaced greyscale bars in terms of emitted light intensity (&lt;a href=&quot;https://github.com/johnnovak/johnnovak.site/blob/master/blog/files/2016-09-21/src/gammaramp.nim&quot;&gt;Nim source code&lt;/a&gt;)&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Now consider the following image:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/gamma-ramp32.png&quot; alt=&quot;Figure 2 &amp;mdash; Evenly-spaced greyscale bars in terms of perceptual light intensity&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%; text-align: center;&quot;&gt;Figure 2 &amp;mdash; Evenly-spaced greyscale bars in terms of perceptual light intensity (&lt;a href=&quot;https://github.com/johnnovak/johnnovak.site/blob/master/blog/files/2016-09-21/src/gammaramp.nim&quot;&gt;Nim source code&lt;/a&gt;)&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;On which image does the gradation appear more even? It’s the second one! But
why is that so? We have just established that in the first image the bars are
evenly (&lt;em&gt;linearly&lt;/em&gt;) spaced in terms of emitted light intensity between the
darkest black and brightest white your monitor is capable of reproducing. But
why don’t we see that as a nice even gradation from black to white then? And
what is being displayed on the second image that we &lt;em&gt;perceive&lt;/em&gt; as a linear
gradation?&lt;/p&gt;

&lt;p&gt;The answer lies in the response of the human eye to light intensity, which is
&lt;em&gt;non-linear&lt;/em&gt;. One the first image, the &lt;strong&gt;difference&lt;/strong&gt; between the nominal light
intensity of any two neighbouring bars is constant:&lt;/p&gt;

&lt;p&gt;$$\Δ_{\linear} = I_n-I_{n-1}$$&lt;/p&gt;

&lt;p&gt;On the second image, however, this difference is not constant but changes from
bar to bar; it follows a power law relationship, to be exact. All human
sensory perception follows a similar &lt;a href=&quot;https://en.wikipedia.org/wiki/Stevens&#39;_power_law&quot;&gt;power law
relationship&lt;/a&gt; in terms of
the magnitude of stimulus and its perceived intensity.&lt;/p&gt;

&lt;p class=&quot;important&quot;&gt;Because of this, we say that there is a &lt;strong&gt;power law relationship&lt;/strong&gt; between
&lt;strong&gt;nominal physical light intensity&lt;/strong&gt; and &lt;strong&gt;perceptual brightness&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;physical-vs-perceptual-linearity&quot;&gt;Physical vs perceptual linearity&lt;/h3&gt;

&lt;p&gt;Let’s say we wanted to store a representation of the following real-world
object as an image file on the computer (let’s pretend for a moment
that perfect greyscale gradients exist in the real world, okay?) Here’s how
the “real world object” looks like:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/linear-ramp.png&quot; alt=&quot;Figure 3 &amp;mdash; Ideal smooth greyscale ramp&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%; text-align: center;&quot;&gt;Figure 3 &amp;mdash; Ideal smooth greyscale ramp (&lt;a href=&quot;https://github.com/johnnovak/johnnovak.site/blob/master/blog/files/2016-09-21/src/gammaramp.nim&quot;&gt;Nim source code&lt;/a&gt;)&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Now, let’s pretend that we can only store 5-bit greyscale images on this
particular computer system, which gives us 32 distinct shades of grey ranging
from absolute black to absolute white. Also, on this computer, greyscale
values are &lt;em&gt;proportional&lt;/em&gt; with their corresponding physical light intensities,
which will result in a 32-element greyscale as shown on Figure 1. We can say
that this greyscale is &lt;em&gt;linear&lt;/em&gt; in terms of &lt;em&gt;light emission&lt;/em&gt; between
successive values.&lt;/p&gt;

&lt;p&gt;If we encoded our smooth gradient using only these 32 grey values, we would get
something like this (let’s just ignore dither for now to keep things simple):&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/linear-ramp32-perceptual.png&quot; alt=&quot;Figure 4 &amp;mdash; Ideal smooth greyscale ramp represented with 32 physically-linear greyscale values&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%; text-align: center;&quot;&gt;Figure 4 &amp;mdash; Ideal smooth greyscale ramp represented with 32 physically-linear greyscale values (&lt;a href=&quot;https://github.com/johnnovak/johnnovak.site/blob/master/blog/files/2016-09-21/src/gammaramp.nim&quot;&gt;Nim source code&lt;/a&gt;)&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Well, the transitions are rather abrupt, especially on the left side, because
we only had 32 grey values to work with. If we squint a little, it’s easy to
convince ourselves that this is a more or less “accurate” representation of
the smooth gradient, as far as our limited bit-depth allows it. But note how
the steps are much larger on the left side than on the right—this is because
we are using a greyscale that is &lt;em&gt;linear&lt;/em&gt; in terms of &lt;em&gt;emitted light
intensity&lt;/em&gt;, but as we have mentioned before, our eyes don’t perceive light
intensity in a linear way!&lt;/p&gt;

&lt;p&gt;This observation has some interesting implications. The error between the
original and the 5-bit encoded version is uneven across the image; it’s much
larger for dark values than for light ones. In other words, we are losing
representational precision for dark values and are using relatively too much
precision for lighter shades.  Clearly, we’d be better off choosing
a different set of 32 greys for our limited palette of shades that would make
this error evenly distributed across the whole range, so both dark and light
shades would be represented with the same precision. If we encoded our
original image with such a greyscale that is &lt;em&gt;perceptually linear&lt;/em&gt;, but
consequently &lt;em&gt;non-linear&lt;/em&gt; in terms of emitted light intensity, and that
non-linearity would match that of the human vision, we’d get the exact same
greyscale image we have already seen in Figure 2:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/gamma-ramp32.png&quot; alt=&quot;Figure 5 &amp;mdash; Ideal smooth greyscale represented with 32 perceptually-linear greyscale values&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;Figure 5 &amp;mdash; Ideal smooth greyscale represented with 32 perceptually-linear greyscale values (&lt;a href=&quot;https://github.com/johnnovak/johnnovak.site/blob/master/blog/files/2016-09-21/src/gammaramp.nim&quot;&gt;Nim source code&lt;/a&gt;)&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p class=&quot;important&quot;&gt;The non-linearity we’re talking about here is the &lt;strong&gt;power law&lt;/strong&gt; relationship
we mentioned before, and the non-linear transformation we need to apply to our
&lt;em&gt;physically linear&lt;/em&gt; greyscale values to transform them into &lt;em&gt;perceptually
linear&lt;/em&gt; values is called &lt;strong&gt;gamma correction&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;efficient-image-encoding&quot;&gt;Efficient image encoding&lt;/h3&gt;

&lt;p&gt;Why is the all the above important? Colour data in so-called “true colour” or
“24-bit” bitmap images is stored as three 8-bit integers per pixel.  With
8 bits, 256 distinct intensity levels can be represented, and if the spacing
of these levels were physically linear, we would be losing a lot of precision
on dark shades while being unnecessarily precise on light shades (relatively
speaking), as shown above.&lt;/p&gt;

&lt;p&gt;Clearly, this is not ideal. One solution would be to simply keep using the
physically linear scale and increase the bit depth per channel to 16 (or
more).  This would double the storage requirements (or worse), which was not
an option when most common image formats were invented. Therefore, a different
approach was taken. The idea was to let the 256 distinct levels represent
intensity values on a perceptually linear scale instead, in which case the
vast majority of images could be adequately represented on just 8 bits per
colour channel.&lt;/p&gt;

&lt;p class=&quot;important&quot;&gt;The transformation used to represent the &lt;em&gt;physically linear&lt;/em&gt; intensity data
either generated synthetically via an algorithm or captured by a linear device
(such as a CMOS of a digital camera or a scanner) with the discrete values of
the &lt;em&gt;perceptually linear&lt;/em&gt; scale is called &lt;strong&gt;gamma encoding&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The 24-bit &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model#Video_framebuffer&quot;&gt;RGB colour
model&lt;/a&gt;
(RGB24) used on virtually all consumer level electronic devices uses 8-bit
&lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model#Nonlinearity&quot;&gt;gamma encoded
values&lt;/a&gt; per
channel to represent light intensities. If you recall what we discussed
earlier, this means that pixels with RGB(128, 128, 128) will &lt;em&gt;not&lt;/em&gt; emit
approximately 50% the light energy of pixels with RGB(255, 255, 255), but only
about 22%! That makes perfect sense! Because of the non-linear nature of human
vision, a light source needs to be attenuated to about 22% of its original
light intensity to appear half as bright to humans.  RGB(128, 128, 128)
&lt;em&gt;appears&lt;/em&gt; to be half as bright as RGB(255, 255, 255) to us! If you find this
confusing, reflect a bit on it because it’s crucial to have a solid
understanding of what has been discussed so far (trust me, it will only get
more confusing).&lt;/p&gt;

&lt;p&gt;Of course, gamma encoding is always done with the assumption that the image is
ultimately meant to be viewed by humans on computer screens. In some way, you
can think of it as a lossy MP3 like compression but for images. For other
purposes (e.g. scientific analysis or images meant for further
post-processing), using floats and sticking with the linear scale is often
a much better choice, as we’ll later see.&lt;/p&gt;

&lt;h3 id=&quot;the-gamma-transfer-function&quot;&gt;The gamma transfer function&lt;/h3&gt;

&lt;p&gt;The process of converting values from linear space to gamma space is called
&lt;strong&gt;gamma encoding&lt;/strong&gt; (or &lt;em&gt;gamma compression&lt;/em&gt;), and the reverse &lt;strong&gt;gamma
decoding&lt;/strong&gt; (or &lt;em&gt;gamma expansion&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;The formulas for these two operations are very simple, we only need to use the
aforementioned power law function:&lt;/p&gt;

&lt;p&gt;$$\V_{\encoded} = \V_{\linear} ^ {1/\γ}$$&lt;/p&gt;

&lt;p&gt;$$\V_{\linear} = \V_{\encoded} ^ {\γ}$$&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;standard gamma (γ)&lt;/strong&gt; value to use in computer display systems is
&lt;strong&gt;2.2&lt;/strong&gt;. The main greason for this is because a gamma of 2.2 approximately
matches the power law sensitivity of human vision. The exact value that should
be used varies from person to person and also depends on the lighting
conditions and other factors, but a standard value had to be chosen and 2.2
was good enough. Don’t be too hung up on this.&lt;/p&gt;

&lt;p&gt;Now, a very important point that many texts fail to mention is that the input
values have to be in the 0 to 1 range and the output will be consequently
mapped to the same range too. From this follows the slightly counter-productive
fact that gamma values between 0 and 1 are used for &lt;strong&gt;encoding&lt;/strong&gt;
(compression) and greater than 1 for &lt;strong&gt;decoding&lt;/strong&gt; (expansion).
The below charts demonstrate the gamma transfer functions for encoding and
decoding, plus the trivial linear gamma (γ=1.0) case:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/gamma.svg&quot; alt=&quot;Figure 6 &amp;mdash; Gamma transfer functions&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%; text-align: center;&quot;&gt;Figure 6 &amp;mdash; Gamma transfer functions (&lt;a href=&quot;https://github.com/johnnovak/johnnovak.site/blob/master/blog/files/2016-09-21/src/gammatransfer.nim&quot;&gt;Nim source code&lt;/a&gt;)&lt;br /&gt;&lt;br /&gt; a) encoding gamma, or gamma compression (γ=1/2.2≈0.4545)&lt;br /&gt;b) linear gamma (γ=1.0)&lt;br /&gt;c) decoding gamma, or gamma expansion (γ=2.2)&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;We have only seen greyscale examples so far, but there’s nothing special about
RGB images—we just simply need to encode or decode each colour channel
individually using the same method.&lt;/p&gt;

&lt;h3 id=&quot;gamma-vs-srgb&quot;&gt;Gamma vs sRGB&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SRGB&quot;&gt;sRGB&lt;/a&gt; is a colour space that is the
de-facto standard for consumer electronic devices nowadays, including
monitors, digital cameras, scanners, printers and handheld devices. It is
also the standard colour space for images on the Internet.&lt;/p&gt;

&lt;p&gt;The sRGB specification defines what gamma to use for encoding and decoding
sRGB images (among other things such as colour gamut, but these are not
relevant to our current discussion). sRGB gamma is very close to a standard
gamma of 2.2, but it has a short linear segment in the very dark range to
avoid a slope of infinity at zero (this is more convenient in numeric
calculations). The formulas to convert from linear to sRGB and back can be
found
&lt;a href=&quot;https://en.wikipedia.org/wiki/SRGB#Specification_of_the_transformationhere&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You don’t actually need to understand all these finer details; the important
thing to know is that in 99% of the cases you’ll want to use sRGB instead of
plain gamma. The reason for this is that all graphics cards have hardware sRGB
support since 2005 or so, so decoding and encoding is virtually for free most
of the time. The native colour space of your monitor is most likely sRGB
(unless it’s a professional monitor for graphics, photo or video work) so if
you just chuck an sRGB encoded pixel data into the framebuffer, the resulting
image will look correct on the screen (given the monitor is properly
calibrated). Popular image formats such as JPEG and PNG can store colour space
information, but very often images don’t contain such data, in which case
virtually all image viewers and browsers will interpret them as sRGB by
convention.&lt;/p&gt;

&lt;h3 id=&quot;gamma-calibration&quot;&gt;Gamma calibration&lt;/h3&gt;

&lt;p&gt;We have talked about gamma encoding and decoding so far, but what is &lt;strong&gt;gamma
calibration&lt;/strong&gt; then? I found this bit slightly confusing too, so let me clear
it up.&lt;/p&gt;

&lt;p&gt;As mentioned, 99% of all monitors today use the sRGB colour space natively,
but due to manufacturing inaccuracies most monitors would benefit from some
additional gamma calibration to achieve the best results. Now, if you never
calibrated your monitor, that doesn’t mean that it will not use gamma! That is
simply impossible, most CRT and LCD displays in the past and present have been
designed and manufactured to operate in sRGB.&lt;/p&gt;

&lt;p&gt;Think of gamma calibration as fine tuning. Your monitor will always operate in
sRGB, but by calibrating it (either in the video card driver or on the OS
level) the monitor’s gamma transfer curve will more closely match the ideal
gamma transfer function we discussed earlier. Also, years ago it was possible
to shoot yourself in the foot in various creative ways by applying multiple
gamma correction stages in the graphics pipeline (e.g. video card, OS and
application level), but fortunately this is handled more intelligently
nowadays. For example, on my Windows 7 box, if I turn on gamma calibration in
the NVIDIA Control Panel then the OS level calibration will be disabled and
vice versa.&lt;/p&gt;

&lt;h3 id=&quot;processing-gamma-encoded-images&quot;&gt;Processing gamma-encoded images&lt;/h3&gt;

&lt;p&gt;So, if virtually the whole world defaults to sRGB, what is exactly the
problem?  If our camera writes sRGB JPEG files, we can just decode the JPEG
data, copy it into the framebuffer of the graphics card and the image would be
displayed correctly on our sRGB LCD monitor (where “correctly” means it would
more or less accurately represent the photographed real-world scene).&lt;/p&gt;

&lt;p&gt;The problem will happen in the moment we start running any image processing
algorithms on our sRGB pixel buffer directly. Remember, gamma encoding is
a non-linear transformation and sRGB encoding is basically just a funky way of
doing gamma encoding of around γ=1/2.2. Virtually all image processing
algorithms you will find in any computer graphics text will assume pixel data
with  &lt;em&gt;linearly encoded light intensities&lt;/em&gt;, which means that feeding these
algorithms with sRGB encoded data will render the results subtly—or in
some cases quite obviously—wrong! This includes resizing, blurring,
compositing, interpolating between pixel values, antialiasing and so on, just
to name the most common operations!&lt;/p&gt;

&lt;h2 id=&quot;effects-of-gamma-incorrectness&quot;&gt;Effects of gamma-incorrectness&lt;/h2&gt;

&lt;p&gt;Alright, enough theory talk, show me how these errors actually look like!
That’s exactly what we’ll do in this section; we will examine the most common
scenarios when running image processing algorithms directly on sRGB data would
manifest in incorrect results. Apart from illustrative purposes, these
examples are also useful for spotting gamma-incorrect behaviour or bugs in
drawing programs and image processing libraries.&lt;/p&gt;

&lt;p&gt;It must be noted that I have chosen examples that clearly demonstrate the
problems with gamma-incorrectness. In most cases, the issues are the most
obvious when using vivid, saturated colours. With more muted colours, the
differences might be less noticeable or even negligible &lt;em&gt;in some cases&lt;/em&gt;.
However, the errors are always present, and image processing programs should work
correctly for all possible inputs, not just okayish for 65.23% of all possible
images… Also, in the area of physically based rendering gamma correctness is
an absolute must, as we’ll see.&lt;/p&gt;

&lt;h3 id=&quot;gradients&quot;&gt;Gradients&lt;/h3&gt;

&lt;p&gt;The image below shows the difference between gradients calculated in linear
(top gradient) and sRGB space (bottom gradient). Note how direct interpolation
on the sRGB values yields much darker and sometimes more saturated looking
images.&lt;/p&gt;

&lt;p&gt;Just going by the looks, one might prefer the look of the sRGB-space versions,
especially for the last two. However, that’s not how light would behave
in the real world (imagine two coloured light sources illuminating a white
wall; the colours would mix as in the linear-space case).&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/gradient.png&quot; alt=&quot;Figure 7 &amp;mdash; For each gradient-pair, the top gradient is a linear interpolation between two colours in linear space, then the result converted to sRGB (gamma-correct). The bottom gradients are the results of interpolating between the exact same colours but directly in sRGB space (gamma-incorrect)&quot; style=&quot;width: 80%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 80%; text-align: center;&quot;&gt;Figure 7 &amp;mdash; For each gradient-pair, the top gradient is a linear interpolation between two colours in linear space, then the result converted to sRGB (gamma-correct). The bottom gradients are the results of interpolating between the exact same colours but directly in sRGB space (gamma-incorrect)&lt;br /&gt;(&lt;a href=&quot;https://github.com/johnnovak/johnnovak.site/blob/master/blog/files/2016-09-21/src/gradient.nim&quot;&gt;Nim source code&lt;/a&gt;).&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Almost everybody does this the wrong way: CSS gradients and transitions are
wrong (see &lt;a href=&quot;https://lists.w3.org/Archives/Public/www-style/2012Jan/0607.html&quot;&gt;this
thread&lt;/a&gt; for
details), Photoshop is wrong (as of version CS6) and there’s not even an option
to fix it.&lt;/p&gt;

&lt;p&gt;Two drawing programs that got this (and gamma-correctness in general) right
are &lt;a href=&quot;https://krita.org/&quot;&gt;Krita&lt;/a&gt; and &lt;a href=&quot;http://www.pixelmator.com/&quot;&gt;Pixelmator&lt;/a&gt;.
SVG also &lt;a href=&quot;https://www.w3.org/TR/SVG/painting.html#ColorInterpolationProperties&quot;&gt;let’s the user to
specify&lt;/a&gt;
whether to use linear or sRGB-space interpolations for gradients, compositing
and animations.&lt;/p&gt;

&lt;h3 id=&quot;colour-blending&quot;&gt;Colour blending&lt;/h3&gt;

&lt;p&gt;Drawing with soft brushes in gamma-incorrect drawing programs can result in
weird darkish transition bands with certain vivid colour combinations.
This is really a variation of the gradient problem if you think about it (the
transition band of a soft brush is nothing else than a small gradient).&lt;/p&gt;

&lt;p&gt;Some random people claimed on the Adobe forums that by doing this Photoshop is
really mimicking how mixing paints would work in real life. Well, no, it
has nothing to do with that. It’s just the result of naive programming to work
directly on the sRGB pixel data and now we’re stuck with that as the default
legacy behaviour.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/color-blending.jpg&quot; alt=&quot;Figure 8 &amp;mdash; Effects of gamma-incorrect colour blending. On the left gamma-correct image, the option &#39;Blend RGB Colors Using Gamma 1.0&#39; was enabled in Photoshop CS6, on the right it was disabled (that&#39;s the default gamma-incorrect legacy mode).&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot;&quot;&gt;Figure 8 &amp;mdash; Effects of gamma-incorrect colour blending. On the left gamma-correct image, the option &#39;Blend RGB Colors Using Gamma 1.0&#39; was enabled in Photoshop CS6, on the right it was disabled (that&#39;s the default gamma-incorrect legacy mode).&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;alpha-blending--compositing&quot;&gt;Alpha blending / compositing&lt;/h3&gt;

&lt;p&gt;As another variation on colour blending, let’s see how alpha blending holds
up. We’ll examine some coloured rectangles first. As expected, the
gamma-correct image on the left mimics how light would behave in real life,
while the sRGB space blending on the right exhibits some weird hue and
brightness shifts.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/opacity.png&quot; alt=&quot;Figure 9 &amp;mdash; Effects of gamma-incorrect alpha blending. For each vertical bar pair, the top one is drawn with 100% opacity and bottom one with 50%. The left image is the gamma-correct one. The test was done in Photoshop CS6, similarly to Figure 8.&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot;&quot;&gt;Figure 9 &amp;mdash; Effects of gamma-incorrect alpha blending. For each vertical bar pair, the top one is drawn with 100% opacity and bottom one with 50%. The left image is the gamma-correct one. The test was done in Photoshop CS6, similarly to Figure 8.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;The appearance of false colours is also noticeable when blending two photos
together. On the gamma-correct image on the left, the skin tones and the reds
and yellows are preserved but faded into the blueish image in a natural way,
while on the right image there’s a noticeable overall greenish cast. Again,
this might be an effect you like, but it’s not how accurate alpha
compositing should work.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-09-21/blending.jpg&quot; data-width=&quot;1260&quot; data-height=&quot;862&quot;&gt;
      &lt;img src=&quot;/files/2016-09-21/blending.jpg&quot; alt=&quot;Figure 10 &amp;mdash; Effects of gamma-incorrectness when compositing photographic images (by yours truly). The two original images from the top row are laid on top of each other in the bottom row, with the blueish image on top having 60% opacity. The left image is the gamma-correct one. Tested with Photoshop CS6. (Click on the image to enlarge it to see the details better.)&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot;&quot;&gt;Figure 10 &amp;mdash; Effects of gamma-incorrectness when compositing photographic images (by &lt;a href=&quot;http://photo.johnnovak.net/&quot;&gt;yours truly&lt;/a&gt;). The two original images from the top row are laid on top of each other in the bottom row, with the blueish image on top having 60% opacity. The left image is the gamma-correct one. Tested with Photoshop CS6. (Click on the image to enlarge it to see the details better.)&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;image-resizing&quot;&gt;Image resizing&lt;/h3&gt;

&lt;p&gt;These examples will only work if your browser doesn’t do any rescaling on the
images below. Also, note that screens of mobile devices are more inaccurate
with regards to gamma than regular monitors, so for best results try to view
this on a desktop computer.&lt;/p&gt;

&lt;p&gt;The image below contains a simple black and white checkerboard pixel pattern
(100% zoom on the left, 400% zoom on the right). The black pixels are
RGB(0,0,0), the minimum light intensity your monitor is capable of producing,
and the white ones RGB(255,255,255), which is the maximum intensity. Now, if
you squint a little, your eyes will blur (average) the light coming from the
image, so you will see a grey that’s halfway in intensity between absolute
black and white (therefore it’s referred to as &lt;strong&gt;50% grey&lt;/strong&gt;).&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/resize-large.png&quot; alt=&quot;Figure 11 &amp;mdash; Black and white checkerboard pixel pattern frequently used in simple gamma calibration programs. The averaged light emission of the area occupied by the pattern is equal to that of a solid 50% grey square. The right image shows the pattern at 400% magnification.&quot; style=&quot;width: 220px&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 80%; text-align: center;&quot;&gt;Figure 11 &amp;mdash; Black and white checkerboard pixel pattern frequently used in simple gamma calibration programs. The averaged light emission of the area occupied by the pattern is equal to that of a solid 50% grey square. The right image shows the pattern at 400% magnification.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;From this follows that if we resized the image by 50%, a similar averaging
process should happen, but now algorithmically on the pixel data. We expect
to get a solid rectangle filled with the same 50% grey that we saw when we
squinted.&lt;/p&gt;

&lt;p&gt;Let’s try it out! On the image below, &lt;em&gt;A&lt;/em&gt; is the checkerboard pattern, &lt;em&gt;B&lt;/em&gt; the
result of resizing the pattern by 50% directly in sRGB-space (using bicubic
interpolation), and &lt;em&gt;C&lt;/em&gt; the resizing it in linear space, then converted to
sRGB.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/resize.png&quot; alt=&quot;Figure 12 &amp;mdash; Effects of gamma-incorrect image resizing. &amp;lt;em&amp;gt;A&amp;lt;/em&amp;gt; is the pixel checkerboard pattern, &amp;lt;em&amp;gt;B &amp;lt;/em&amp;gt;the gamma-incorrect result of resizing the image in sRGB space (Photoshop CS6 in 8-bit RGB mode), and &amp;lt;em&amp;gt;C&amp;lt;/em&amp;gt; the gamma-correct result of converting the image to linear space before resizing, then back to sRGB at the end (Photoshop CS6 in 32-bit RGB mode).&quot; style=&quot;width: 316px&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 80%; text-align: center;&quot;&gt;Figure 12 &amp;mdash; Effects of gamma-incorrect image resizing. &lt;em&gt;A&lt;/em&gt; is the pixel checkerboard pattern, &lt;em&gt;B &lt;/em&gt;the gamma-incorrect result of resizing the image in sRGB space (Photoshop CS6 in 8-bit RGB mode), and &lt;em&gt;C&lt;/em&gt; the gamma-correct result of converting the image to linear space before resizing, then back to sRGB at the end (Photoshop CS6 in 32-bit RGB mode).&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Unsurprisingly, &lt;em&gt;C&lt;/em&gt; gives the correct result, but the shade of grey might not
be an exact match for the blurred checkerboard pattern on your monitor if
it’s not properly gamma-calibrated. Even the math shows this clearly: a 50%
grey pixel that emits half as much light as a white pixel should have a RGB
value of around (186,186,186), gamma-encoded:&lt;/p&gt;

&lt;p&gt;$$0.5^{1\/2.2} ≈ 0.72974$$
$$0.72974·255 = 186$$&lt;/p&gt;

&lt;p&gt;(Don’t worry that on the image the 50% grey is RGB(187,187,187). That small
difference is because the image is sRGB-encoded, but I used the much simpler
gamma formula for my calculation here.)&lt;/p&gt;

&lt;p&gt;Gamma-incorrect resizing can also result in weird hue shifts on some images.
For more details, read &lt;a href=&quot;http://www.4p8.com/eric.brasseur/gamma.html&quot;&gt;Eric Brasseur’s excellent
article&lt;/a&gt; on the matter.&lt;/p&gt;

&lt;h3 id=&quot;antialiasing&quot;&gt;Antialiasing&lt;/h3&gt;

&lt;p&gt;I guess it’s no surprise at this point that antialiasing it no exception when
it comes to gamma-correctness. Antialiasing in γ=2.2 space results in overly
dark “smoothing pixels” (right image); the text appears too heavy, almost as if
it was bold. Running the algorithm in linear space produces much better
results (left image), although in this case the font looks a bit too thin.
Interestingly, Photoshop antialiases text using γ=1.42 by default, and this
indeed seems to yield the best looking results (middle image). I don’t know
the explanation for this, but the take-away point is that antialaising in sRGB
space is almost never what you want.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/antialias.png&quot; alt=&quot;Figure 13 &amp;mdash; Effects of gamma-incorrectness on text antialiasing. The left image was rendered with the option &#39;Blend Text Colors Using Gamma&#39; set to 1.0, the middle one with 1.45, and the right one with 2.2.&quot; style=&quot;width: 623px&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 623px;&quot;&gt;Figure 13 &amp;mdash; Effects of gamma-incorrectness on text antialiasing. The left image was rendered with the option &#39;Blend Text Colors Using Gamma&#39; set to 1.0, the middle one with 1.45, and the right one with 2.2.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;physically-based-rendering&quot;&gt;Physically-based rendering&lt;/h3&gt;

&lt;p&gt;If there’s a single area where gamma-correctness is an absolute must, that’s
physically-based rendering (PBR). To obtain realistic looking results, gamma
should be handled correctly throughout the whole graphics pipeline. There’s
so many ways to screw this up, but these are the two most common ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Doing the calculations in linear space but failing to convert the final
image to sRGB and then “tweaking” various material and lighting parameters
to compensate.&lt;/li&gt;
  &lt;li&gt;Failing to convert sRGB texture images to linear space (or set the sRGB flag
when hardware acceleration is used).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These two basic errors are then usually combined in various interesting ways,
but the end result would invariably fail to resemble a realistic looking scene
(e.g. quadratic light falloff will not appear quadratic anymore, highlights
will be overblown and will exhibit some weird hue and saturation shifts etc.)&lt;/p&gt;

&lt;p&gt;To demonstrate the first mistake using my own &lt;a href=&quot;/tags/ray%20tracing&quot;&gt;ray tracer&lt;/a&gt;, the left image below
shows a very simple but otherwise quite natural looking image in terms of
physical lighting accuracy. This rendering took place in linear space and then
the contents of the framebuffer were converted to sRGB before writing it to
disk.&lt;/p&gt;

&lt;p&gt;On the right image, however, this last conversion step was omitted and I tried
to tweak the light intensities in an attempt to match the overall brightness
of the gamma-correct image. Well, it’s quite apparent that this is not going
to work. Everything appears too contrasty and oversaturated, so we’d probably
need to desaturate all material colours a bit maybe use some more fill lights
to come closer to the look of the left image. But this is a losing battle; no
amount of tweaking will make the image correct in the physical sense, and even
if we got it to an acceptable level for one particular scene with a particular
lighting setup, any further changes to the scene would potentially necessitate
another round of tweaks to make the result look realistic again.  Even more
importantly, the material and lighting parameters we would need to choose
would be completely devoid of any physical meaning whatsoever; they’ll be just
a random set of numbers that happen to produce an OK looking image &lt;em&gt;for that
particular scene&lt;/em&gt;, and thus not transferable to other scenes or lighting
conditions. It’s a lot of wasted energy to work like that.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-09-21/badgamma.png&quot; data-width=&quot;1237&quot; data-height=&quot;400&quot;&gt;
      &lt;img src=&quot;/files/2016-09-21/badgamma.png&quot; alt=&quot;TODO&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 95%; text-align: center;&quot;&gt;Figure 14 &amp;mdash; Effects of gamma-incorrect rendering on diffuse spheres. The gamma-incorrect image on the right shows an unsuccesful attempt at matching the look of the gamma-correct one on the left by tweaking the light intensities alone.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;It’s also important to point out that incorrect gamma handling in 3D rendering
is one of the main culprits behing the “fake plasticky CGI look” in some
(mostly older) games. As illustrated on the image below, rendering realistic
looking human skin is almost impossible with a gamma-incorrect workflow; the
highlights will just never look right. This gave birth to questionable
practices such as compensating for the wrong highlights in the specular maps
with inverted hues and all sorts of other nastiness instead of fixing the
problem right at the source…&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-09-21/lineargammahead.png&quot; alt=&quot;TODO&quot; style=&quot;width: 90%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 90%; text-align: center;&quot;&gt;Figure 15 &amp;mdash; Effects of gamma-incorrect rendering on a human head. On the top, a real looking human head; on the bottom, a wax figure approximation.&lt;br /&gt;(Image from the &lt;a href=&quot;https://docs.unity3d.com/Manual/LinearLighting.html&quot;&gt;Unity3D manual&lt;/a&gt;.)&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is pretty much all there is to gamma encoding and decoding.
Congratulations for making it so far, now you’re an officially certified
gamma-compliant developer! :)&lt;/p&gt;

&lt;p&gt;To recap, the only reason to use gamma encoding for digital images is because
it allows us to store images more efficiently on a limited bit-length. It
takes advantage of a characteristic of human vision that we perceive
brightness in a logarithmic way. Most image processing algorithms expect pixel
data with linearly encoded light intensities, therefore gamma-encoded images
need to be gamma-decoded (converted to linear space) first before we can run
these algorithms on them. Often the results need to be converted back to
gamma-space to store them on disk or to display them on graphics hardware that
expects gamma-encoded values (most consumer-level graphics hardware fall into
this category). The de-facto standard sRGB colourspace uses a gamma of
approximately 2.2. That’s the default colourspace for images on the Internet
and for most monitors, scanners and printers. When in doubt, just use sRGB.&lt;/p&gt;

&lt;p&gt;From the end-user perspective, keep in mind that most applications and
software libraries do not handle gamma correctly, therefore always make sure
to do extensive testing before adopting them into your workflow. For a proper
linear workflow, &lt;em&gt;all&lt;/em&gt; software used in the chain has to be 100%
gamma-correct.&lt;/p&gt;

&lt;p&gt;And if you’re a developer working on graphics software, please make sure
you’re doing the correct thing. Be gamma-correct and always explicitly state
your assumptions about the input and output colour spaces in the software’s
documentation.&lt;/p&gt;

&lt;p&gt;May all your lights be linear! :)&lt;/p&gt;

&lt;hr /&gt;

&lt;section class=&quot;links&quot;&gt;

  &lt;h2 id=&quot;references--further-reading&quot;&gt;References &amp;amp; further reading&lt;/h2&gt;

  &lt;h3 id=&quot;general-gammasrgb-info&quot;&gt;General gamma/sRGB info&lt;/h3&gt;

  &lt;ul class=&quot;compact&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.poynton.com/GammaFAQ.html&quot;&gt;Charles Poynton – Gamma FAQ&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.cambridgeincolour.com/tutorials/gamma-correction.htm&quot;&gt;Cambridge in Colour – Understanding gamma correction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://gamedevdaily.io/the-srgb-learning-curve-773b7f68cf7a#.ssgyxju0h&quot;&gt;Tom Forsynth – The sRGB Learning Curve&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://ninedegreesbelow.com/photography/linear-gamma-blur-normal-blend.html&quot;&gt;Nine Degrees Below – Linear Gamma vs Higher Gamma RGB Color Spaces: Gaussian Blur and Normal Blend Mode&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.4p8.com/eric.brasseur/gamma.html&quot;&gt;Eric Brasseur – Gamma error in picture scaling&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_correction&quot;&gt;Wikipedia – Gamma correction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SRGB&quot;&gt;Wikipedia – sRGB&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;&gt;Wikipedia – RGB color model&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

  &lt;h3 id=&quot;linear-lighting--workflow-lwf&quot;&gt;Linear lighting &amp;amp; workflow (LWF)&lt;/h3&gt;

  &lt;ul class=&quot;compact&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;https://frictionalgames.blogspot.com/2013/11/tech-feature-linear-space-lighting.html&quot;&gt;In the Games of Madness – Tech Feature: Linear-space lighting&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html&quot;&gt;Larry Gritz, GPU Gems 3 – Chapter 24. The Importance of Being Linear&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.peachpit.com/articles/article.aspx?p=2165641&quot;&gt;Jeremy Birn – Top Ten Tips for More Convincing Lighting and Rendering&lt;/a&gt; – (Section 1. Use a Linear Workflow)&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://docs.unity3d.com/Manual/LinearLighting.html&quot;&gt;Unity Documentation – Linear Rendering&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://spitzak.github.io/conversion/index.html&quot;&gt;Bill Spitzak – High-speed Conversion of Floating Point Images to 8-bit&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://renderman.pixar.com/view/LinearWorkflow&quot;&gt;Renderman – Linear Workflow&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://greyscalegorilla.com/tutorials/what-is-linear-workflow-and-how-can-it-help-your-renders-look-better/&quot;&gt;Nick Campbell – What Is Linear Workflow and How Can It Help Your Renders Look Better?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://blackhole12.blogspot.com.au/2011/12/great-mystery-of-linear-gradient.html&quot;&gt;Eric McClure – The Great Mystery of Linear Gradient Lighting&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

  &lt;h3 id=&quot;bonus-stuff&quot;&gt;Bonus stuff&lt;/h3&gt;

  &lt;ul class=&quot;compact&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.rle.mit.edu/stir/documents/VarshneyS_Significance2013.pdf&quot;&gt;Lav R. Varshney, John Z. Sun – Why do we perceive logarithmically? [PDF]&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Stevens&#39;_power_law&quot;&gt;Wikipedia – Stevens’ power law&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://userbase.kde.org/Krita/Manual/ColorManagement&quot;&gt;Krita Manual – Color management&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://ptgmedia.pearsoncmg.com/imprint_downloads/peachpit/peachpit/lightroom4/pdf_files/LightroomRGB_Space.pdf&quot;&gt;The Adobe Photoshop Lightroom Book – The Lightroom RGB space [PDF]&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.openexr.com/&quot;&gt;OpenEXR&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://cinematiccolor.com/&quot;&gt;Cinematic Color: Motion-Picture Color Management&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/section&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:osgamma&quot;&gt;
      &lt;p&gt;Only if your operating system is Mac OS X 10.6 or higher or Linux. &lt;a href=&quot;#fnref:osgamma&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
                <pubDate>Wed, 21 Sep 2016 00:00:00 +1000</pubDate>
                <link>http://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/</link>
                <guid isPermaLink="true">http://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/</guid>
            </item>
        
            <item>
                <title>Cross-platform GUI Toolkit Trainwreck, 2016 Edition</title>
                <description>&lt;p class=&quot;intro&quot;&gt;Ok, I’m not too sure in what direction this post will go, an informative
article, a rant, or both. Probably a bit of both, with an emphasis on the rant
part, given my current not-quite-positive emotional involvement with the
topic. Gentlemen (and gentlewomen), please fasten your seatbelts!&lt;/p&gt;

&lt;p class=&quot;intro&quot;&gt;(UPDATE: It turned out to be a quite informative mega-post in the end, just
don’t give up reading after the first section…)&lt;/p&gt;

&lt;h2 id=&quot;buttons-and-pixels&quot;&gt;Buttons and pixels&lt;/h2&gt;

&lt;p&gt;In 2016, developing a program in entirety on one particular platform, for
example OS X, and then successfully compiling and running it on another, say
Windows, with zero or minimal modifications required, is no longer an
utopistic dream. In fact, users of dynamic, interpreted or scripting languages
(however you like to call them today) have been enjoying the luxuries of easy
cross-platform development for many decades now; they won’t even raise an
eyebrow on such trivial matters. But even the lowly C and C++ plebs are able
to perform this feat with relative ease nowadays—at least, if they stuck
to their respective language standards religiously and turned their compiler
warnings up to nuclear-strength…&lt;/p&gt;

&lt;p&gt;Being able to use a single codebase for multi-platform applications is of
crucial importance. After all, who wants to maintain two or three (or more)
codebases that are mostly similar, but &lt;em&gt;not quite&lt;/em&gt;? I sure don’t. And then
there’s the convenience and flexibility factor too: you can happily develop on
your Linux VM on your desktop PC at home, then maybe fix some bugs on your
MacBook while sitting on the train, with the confidence that your program will
work flawlessly on Windows too. Not having to deal with platform differences
and idiosyncrasies takes a huge burden off of developers’ shoulders that is
not to be underestimated.&lt;/p&gt;

&lt;p&gt;Some compiled languages, like &lt;a href=&quot;http://nim-lang.org/&quot;&gt;Nim&lt;/a&gt;, give you such
cross-platform guarantees by design. This is all good, this is all well, this
is precisely how things should be. The world is such a wonderful place, after
all. Write once, run anywhere (and don’t go near the JVM). Excellent! The
universe is smiling at you. Infinity co creates karmic balance. Freedom
unfolds into intrinsic actions. Your consciousness manifests through quantum
reality. &lt;sup id=&quot;fnref:bullshit&quot;&gt;&lt;a href=&quot;#fn:bullshit&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Except for one little thing. You must not under any circumstance try to open
a window (on the computer, I mean), attempt to change the colour of a single
pixel in it, or—god forbid!—fantasise about using native (or any kind of,
for the matter) GUI controls in a cross-platform (and non-hair loss inducing)
manner! If you disregarded my sage advice and foolishly ventured to accomplish
any of the above tasks, the bliss would be over in an instant and you’d find
yourself rudely transported back to the early days of computing where making
anything just simply work &lt;em&gt;at all&lt;/em&gt; on different OSs would take inordinate
amounts of time and effort!&lt;/p&gt;

&lt;h2 id=&quot;solution-to-a-problem&quot;&gt;Solution to a problem&lt;/h2&gt;

&lt;p&gt;I’m not just talking in the abstract here, I am merely describing my trials
and tribulations when attempting to extend my &lt;a href=&quot;https://github.com/johnnovak/nim-raytracer&quot;&gt;ray
tracer&lt;/a&gt; I’m developing in Nim with
a very minimalistic GUI. My ambitions were fairly modest: I only wanted to
open a window and then split it into a canvas and a control area.  Onto the
canvas I would just blit some pixel data periodically and the control area
would contain some buttons and some static text (maybe a progress bar too if
I felt particularly bold and adventurous!). I wanted this fine piece of
software craftsmanship to work on Windows, Linux and OS X, without having to
write any platform specific code. And that’s it!&lt;/p&gt;

&lt;p&gt;So next step, let’s investigate what cross-platform libraries are available
for Nim!&lt;/p&gt;

&lt;h3 id=&quot;iup&quot;&gt;IUP&lt;/h3&gt;

&lt;p&gt;I was looking for something really minimal and I would have
perfectly been happy with native controls, so the &lt;a href=&quot;https://github.com/nim-lang/gtk2&quot;&gt;Nim
bindings&lt;/a&gt; to the &lt;a href=&quot;http://webserver2.tecgraf.puc-rio.br/iup/&quot;&gt;IUP
GUI&lt;/a&gt; library (a cross-platform
toolkit in C for creating GUI applications in Lua) looked very promising…
until I found out that it does not support OS X at all. Oh well.&lt;/p&gt;

&lt;h3 id=&quot;gtk2&quot;&gt;GTK2&lt;/h3&gt;

&lt;p&gt;The next obvious candidate was the &lt;a href=&quot;https://github.com/nim-lang/gtk2&quot;&gt;GTK2 bindings for
Nim&lt;/a&gt;. I am not a fan of GTK on Windows at
all, and especially not on OS X (despite the fact that I really love
&lt;a href=&quot;https://inkscape.org/&quot;&gt;Inkscape&lt;/a&gt; on Windows, but let’s just write that off as
an anomaly).  Anyway, why not give it shot, it doesn’t cost any money after
all.  Well, after having spent about half an hour foraging on the Internet for
the GTK2 Windows binaries (because the official &lt;a href=&quot;http://www.gtk.org/&quot;&gt;GTK+ Project
website&lt;/a&gt; is of not much help at all in that regard, apart
from some &lt;a href=&quot;http://www.gtk.org/download/windows.php&quot;&gt;kinda vague instructions&lt;/a&gt;
on where to try to locate them), the poor directory containing my ~300K
executable got suddenly about &lt;strong&gt;20 megabytes bigger (!)&lt;/strong&gt; in the form of the
cheerful company of 10+ DLL files. The thing surely worked just fine (to the
extent that GTK2 is capable of doing so), but this is a very bad start already
as I wanted something small that can be statically linked. Adding 20 MB fat
to my cute little 300K renderer is an idea that I find quite obscene to be
honest (as in “obscenely obese”), so in short, no thanks.&lt;/p&gt;

&lt;p&gt;Executive summary: GTK2 + non-Linux platform = AVOID&lt;/p&gt;

&lt;p&gt;Some people might feel the urge to point out now that I’m way too picky, this
is merely a solution to a problem, it works, so I should just suck it up and
live with the &lt;strong&gt;over 60-fold increase of my total binary size!&lt;/strong&gt; Well, this is
a solution to a problem too:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-05-29/headphones.jpg&quot; alt=&quot;Solution to a problem...&quot; style=&quot;width: 70%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 70%;&quot;&gt;Warning: putting up with lots external dependency crap just to make things work somehow is a straight road to this to happen to you!&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;And with that move, I had practically exhausted all the readily available GUI
options for Nim. (Note that this is not a rant against Nim at all;
I absolutely love that language, (in fact, it’s my favourite language at the
moment); it’s not Nim’s fault that most—if not &lt;em&gt;all&lt;/em&gt;—cross platform GUI
toolkits suck in one way or another (having been written in C++ is one major
source of such suckage)). &lt;sup id=&quot;fnref:lisp&quot;&gt;&lt;a href=&quot;#fn:lisp&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;At this point, the needle of my stress-o-meter was already hovering in the
orange zone. Luckily enough, I then found something interesting…&lt;/p&gt;

&lt;h2 id=&quot;imgui-to-the-rescue&quot;&gt;IMGUI to the rescue&lt;/h2&gt;

&lt;p&gt;I haven not heard about immediate mode graphical UIs (IMGUI) before, so in my
initial excitement I thought this could be the answer to all my woes. For
anyone not familiar with the concept, the general idea is that with an IMGUI
the UI does not live in memory and manage it’s own state, as it is the case
with traditional retained mode GUIs (RMGUI), but it gets “recreated” and
redrawn on the fly on every frame at 60 FPS (or whatever your framerate is).
From an implementation perspective, an IMGUI essentially boils down to one
function per widget type, where that single function performs all duties
related to the correct functioning of that particular widget (event handling,
drawing, reporting the current state, etc.) The construction of an IMGUI based
UI thus becomes a series of simple function calls which makes it an attractive
option from a simplicity and iteration speed standpoint. Also, because the
whole interface gets fully redrawn on every frame, there’s no messing with
dirty regions at all—just redraw the whole thing into an off-screen buffer,
overlay it on the top of the current frame and the job’s done. Easy!&lt;/p&gt;

&lt;p&gt;It turns out that while such an approach makes a lot of sense for a game that
needs to redraw the whole screen at a constant framerate anyway, it’s not that
great choice for a traditional desktop applications where such frequent
redraws would be too wasteful.&lt;/p&gt;

&lt;p&gt;(At least, that was my simplistic understanding of the whole IMGUI concept,
which is not quite true, but I’ll just leave the above description here
unaltered, as by judging by some of the threads on the topic, most people seem
to misinterpret the concept of IMGUI in a very similar way like I did when
I was just getting acquainted with it. So if you fully agree with the above
two paragraphs, you’re wrong! :) What I described above is &lt;em&gt;one particular&lt;/em&gt;
IMGUI implementation that is very well suited to applications that need to
redraw the screen at a constant frame rate anyway, generally using accelerated
graphics (read, games). But the actual definition of IMGUI is much simpler:
the UI is just a function of the current application state, the application
should not be responsible for explicitly managing the UI state; keeping it in
sync with the state of the app itself.)&lt;/p&gt;

&lt;h3 id=&quot;enter-nanovg&quot;&gt;Enter NanoVG&lt;/h3&gt;

&lt;p&gt;Ok, so my idea was to build a simple IMGUI user interface myself with
&lt;a href=&quot;https://github.com/memononen/nanovg&quot;&gt;NanoVG&lt;/a&gt;, which is a 2D vector graphics
library that uses OpenGL as its rendering backend. The plan was to use it in
conjunction with GLFW to shield me from any platform specific drawing and
window handling stuff. That part actually worked out quite nicely; after a few
days of hacking I had a window showing a bitmap image which was periodically
updated from the internal render framebuffer, and some GUI elements laid on
top of it with transparency and whatnot.&lt;/p&gt;

&lt;p&gt;So far so good, cross-platform custom GUI proof-of-concept, tick, but
I suddenly found myself facing two new problems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The constant redrawing of the whole UI on every frame was burning too much
CPU, about 10-15% CPU on my Intel Core i7 4790 4.0 GHz (8 logical cores).
That means 1 core out of the total 8 was running at almost 100% all the
time!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The text rendering quality of NanoVG made me really depressed…&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;exit-nanovg&quot;&gt;Exit NanoVG&lt;/h3&gt;

&lt;p&gt;The first problem was easy to fix. The abysmal performance had nothing to do
with NanoVG or OpenGL, it was caused by the constant redrawing at 60 FPS. The
solution was to redraw only when needed: as a quick hack I introduced a global
boolean &lt;code&gt;doRedraw&lt;/code&gt; and set it to true only when an input event was received or
the internal application state has changed (e.g. the framebuffer has been
updated). Then the drawing would only happen when &lt;code&gt;doRedraw&lt;/code&gt; was set to true.
Surely this could be done in a nicer way, but the general concept would be the
same.&lt;/p&gt;

&lt;p&gt;The second issue with the text rendering was a harder nut to crack. NanoVG
uses the tiny
&lt;a href=&quot;https://github.com/nothings/stb/blob/master/stb_truetype.h&quot;&gt;stb_truetype&lt;/a&gt;
single-header C font rasterizer to create the font atlases used for text
output. This is all well, but the quality at small font sizes is not that
great at all (not great enough the annoy the hell out of me). Now, NanoVG
seems to have optional support FreeType, but even if that works, it only has
quite rudimentary support for handling text layouts (which is quite buggy, by
the way). I really have zero inclination to neither start hacking the C code,
nor come up with my own font layout engine… I know there’s stuff like
&lt;a href=&quot;http://www.pango.org/&quot;&gt;Pango&lt;/a&gt; and
&lt;a href=&quot;https://www.freedesktop.org/wiki/Software/HarfBuzz/&quot;&gt;HarfBuzz&lt;/a&gt;, but I really
don’t want to &lt;a href=&quot;https://signalvnoise.com/posts/3183-the-art-of-computer-typography&quot;&gt;do a Donald E.
Knuth&lt;/a&gt;
here and spend too much time on a problem that has already been solved on the
OS graphics library level in a perfectly satisfactory way. I just want to call
&lt;code&gt;drawText()&lt;/code&gt; and be done with it!&lt;/p&gt;

&lt;p&gt;Alright, so OpenGL—and thus NanoVG—is not the way to go because of this
whole text rendering fiasco…  At this point, the progress of my adventures
in the wonderful realm of cross-platform graphical user interfaces could have
been be visualised pretty much spot on as follows:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-05-29/fail.jpg&quot; alt=&quot;FAIL&quot; style=&quot;width: auto&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: auto; text-align: center;&quot;&gt;FAIL&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;state-of-the-art&quot;&gt;State of the art&lt;/h2&gt;

&lt;p&gt;Ok, after putting the problem aside for a few days (mainly in order to calm
down), I realised that the best way to go about this is to take a few quality
cross-platform applications that I know well and analyse how they solved the
custom GUI problem. So that’s exactly what I did next.&lt;/p&gt;

&lt;p&gt;I have included a mini-breakdown for each app on the sizes of their various
components (e.g. main executable, libraries, additional resources etc.), in an
attempt to get a feel for what is considered acceptable in 2016. I have used
the 64-bit Windows versions except where noted otherwise.&lt;/p&gt;

&lt;h3 id=&quot;reaper&quot;&gt;REAPER&lt;/h3&gt;

&lt;table class=&quot;properties&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Version&lt;/td&gt;
      &lt;td&gt;5.12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Main executable&lt;/td&gt;
      &lt;td&gt;11 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Resources&lt;/td&gt;
      &lt;td&gt;23 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Plugins&lt;/td&gt;
      &lt;td&gt;30 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total install size&lt;/td&gt;
      &lt;td&gt;68 MiB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;http://www.reaper.fm/&quot;&gt;REAPER&lt;/a&gt; is a highly advanced cross-platform digital
audio production workstation (DAW) for Windows and Mac OS X developed by the
same guy who wrote the original WinAmp. REAPER is written in C++ and it uses
the open-source &lt;a href=&quot;http://www.cockos.com/wdl/&quot;&gt;WDL&lt;/a&gt; library (from the same
developer) for cross-platform graphics, audio and UI tasks.&lt;/p&gt;

&lt;p&gt;It is a very much no-bullshit app, just look at the 11 MiB executable size! By
examining the WDL sources it becomes pretty clear that REAPER uses a mixture
of native software rendering (GDI on Windows, because it still supports XP)
and an anti-aliased software rasterizer. The GUI is skinnable, but
non-scalable, as the skins are completely bitmap based, so a good proportion
of the GUI drawing consists of blitting operations, presumably.  There are
some font rendering differences between the OS X and Windows versions, which
suggests that the app uses OS native text rendering under the hood.  The menu
bar and all dialog windows (e.g. file dialogs, preferences) are OS-native too.&lt;/p&gt;

&lt;p&gt;It is evident that REAPER’s strategy in terms of the UI is to use as much OS
provided functionality as possible and resort to custom code only when
necessary.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-05-29/reaper.png&quot; data-width=&quot;1920&quot; data-height=&quot;1080&quot;&gt;
      &lt;img src=&quot;/files/2016-05-29/reaper-small.jpg&quot; alt=&quot;REAPER 5 screenshot showing the Funktion 1.0 skin&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;This is REAPER 5 in fullscreen mode, featuring the &lt;a href=&quot;http://stash.reaper.fm/theme/1792/Funktion%201.0&quot;&gt;Funktion 1.0&lt;/a&gt; skin (designed by &lt;em&gt;yours truly&lt;/em&gt;). Note the standard Windows menu bar on top.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;renoise&quot;&gt;Renoise&lt;/h3&gt;

&lt;table class=&quot;properties&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Version&lt;/td&gt;
      &lt;td&gt;3.1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Executable&lt;/td&gt;
      &lt;td&gt;26 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DLL files&lt;/td&gt;
      &lt;td&gt;1.3 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Resources&lt;/td&gt;
      &lt;td&gt;19 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Library (presets, samples)&lt;/td&gt;
      &lt;td&gt;131 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total install size&lt;/td&gt;
      &lt;td&gt;195 MiB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://www.renoise.com/&quot;&gt;Renoise&lt;/a&gt; is probably the best cross-platform modern
tracker in existence today. It runs on Windows, OS X and Linux and it has
a completely custom single-window UI. Everything is custom drawn, including
the menus, dialogs, the file browser and so on. The graphics backend uses
DirectX on Windows, OpenGL on OS X and presumably X directly on Linux. The UI
is non-scalable and non-skinnable, the fonts are bitmap based (I think) and
most of the drawing seems to be simple blitting. They also must have developed
some custom graphics routines for the anti-aliased cross-platform drawing of
dynamic UI elements (e.g. the waveform and envelope displays). The UI seems to
be pixel-identical across platforms.&lt;/p&gt;

&lt;p&gt;Renoise is closed source, so unfortunately I could not inspect how they
accomplished all this, but very likely they had to come up with their own UI
and graphics wrappers to maintain a single codebase for all three platforms.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-05-29/renoise.png&quot; data-width=&quot;1920&quot; data-height=&quot;1080&quot;&gt;
      &lt;img src=&quot;/files/2016-05-29/renoise-small.jpg&quot; alt=&quot;Renoise 3 screenshot&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;Renoise 3 in fullscreen. Everything is custom drawn, such as the menu bar and the preferences dialog in the middle of the screen. Again, the nice looking theme is yet another project of mine.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;tracktion&quot;&gt;Tracktion&lt;/h3&gt;

&lt;table class=&quot;properties&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Version&lt;/td&gt;
      &lt;td&gt;7.1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Main executable&lt;/td&gt;
      &lt;td&gt;57 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DLL files&lt;/td&gt;
      &lt;td&gt;2.3 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total install size&lt;/td&gt;
      &lt;td&gt;60 MiB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://www.tracktion.com/&quot;&gt;Tracktion&lt;/a&gt; is another cross-platform DAW
targeting the Windows, Mac OS X and Linux platforms. The single-window GUI is
fully custom drawn, just like in the case of Renoise, but what sets it apart
from it is that the drawing here is much more dynamic: instead of having
mostly fixed-size UI elements, all widgets in Tracktion shrink and enlarge as
their respective containers change in size.&lt;/p&gt;

&lt;p&gt;Tracktion is written in C++ and it uses the
&lt;a href=&quot;https://github.com/julianstorer/JUCE&quot;&gt;JUCE&lt;/a&gt; library for all cross-platform
duties (again, written by the same single person responsible for all Tracktion
development). JUCE supports anti-aliased vector graphics and text output
through a number of wrapper classes that can use either JUCE’s internal
software rasterizer, or platform specific graphics APIs as their backend (e.g.
Direct2D and DirectWrite on Windows and CoreGraphics on OS X). All
platform-specific event and window handling is abstracted away in a similar
fashion.&lt;/p&gt;

&lt;p&gt;The JUCE library is available for free for non-commercial open-source
projects, but a commercial license will make the wallets of enterprising
developers exactly $999 lighter.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-05-29/tracktion7.jpg&quot; data-width=&quot;1800&quot; data-height=&quot;1013&quot;&gt;
      &lt;img src=&quot;/files/2016-05-29/tracktion7-small.jpg&quot; alt=&quot;Tracktion 7 screenshot, big screen&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;Tracktion 7 in it&#39;s fully anti-aliased single-window glory. Previous versions might have looked somewhat less sleek, but the UI was designed to be highly scalable right from the very first release.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-05-29/tracktion7-lowres.jpg&quot; data-width=&quot;950&quot; data-height=&quot;722&quot;&gt;
      &lt;img src=&quot;/files/2016-05-29/tracktion7-lowres-small.jpg&quot; alt=&quot;Tracktion 7 screenshot, small screen&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;Still Tracktion 7, but now on a smaller screen. Contrast this screenshot with the one above: all common UI elements are still there but their sizes are vastly different. Tracktion has the most adaptable dynamic interface of all applications presented in this article, thanks to JUCE&#39;s extensive anti-aliased vector graphics support. &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;blender&quot;&gt;Blender&lt;/h3&gt;

&lt;table class=&quot;properties&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Version&lt;/td&gt;
      &lt;td&gt;2.77&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Main executable&lt;/td&gt;
      &lt;td&gt;94 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DLL files&lt;/td&gt;
      &lt;td&gt;29 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;53 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Data&lt;/td&gt;
      &lt;td&gt;49 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Scripts&lt;/td&gt;
      &lt;td&gt;34 MB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total install size&lt;/td&gt;
      &lt;td&gt;305 MiB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://www.blender.org/&quot;&gt;Blender&lt;/a&gt; doesn’t need much introduction, being the
most well-known open-source 3D package for Windows, OS X and Linux. On the UI
front it uses a completely different approach to all the previous examples:
the contents of the whole window, including the render view and the user
interface, are drawn using pure OpenGL. There’s no fallback to any
other rendering backends—Blender simply doesn’t run on systems without OpenGL
support.&lt;/p&gt;

&lt;p&gt;This makes certain interesting things possible, such as semi-transparent user
interface elements overlaid on top of OpenGL views, as shown on the screenshot
below.  The UI is fully scalable (including font sizes) and quite dynamic,
and it’s also worth noting that it’s fully defined in the form of Python
scripts (hence the bundled Python interpreter).&lt;/p&gt;

&lt;p&gt;Displaying text elements is accomplished via glyphs pre-rendered into
&lt;a href=&quot;https://en.wikipedia.org/wiki/Texture_atlas&quot;&gt;texture atlases&lt;/a&gt; with
&lt;a href=&quot;https://www.freetype.org/&quot;&gt;FreeType&lt;/a&gt; (possibly a modified version). One of
the biggest weaknesses of OpenGL based UI drawing is the difficulty of
rendering crisp and clear looking anti-aliased text, as evidenced by &lt;a href=&quot;https://wiki.blender.org/index.php/Dev:Source/Text/Rendering&quot;&gt;these
notes&lt;/a&gt; from the
Blender developer wiki.&lt;/p&gt;

&lt;p&gt;In theory, it would be possible to re-use Blender’s cross-platform UI layer
written in C in other applications, but because its tight coupling to
Blender’s internals, no one has been able to do yet so in practice.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-05-29/blender.jpg&quot; data-width=&quot;1920&quot; data-height=&quot;1200&quot;&gt;
      &lt;img src=&quot;/files/2016-05-29/blender-small.jpg&quot; alt=&quot;Blender screenshot&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;Blender&#39;s OpenGL-based interface is quite sleek and modern looking. Notice the semi-transparent widgets on top of the 3D views.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;cinema-4d&quot;&gt;Cinema 4D&lt;/h3&gt;

&lt;p&gt;I cannot provide any detailed info on Cinema 4D because I’m not using it
personally (and I couldn’t bring myself to download the 3 GB demo installer
just to check the file sizes…) It’s still worthwhile to include it in this
discussion because a) the UI looks good, b) most other commercial 3D packages
follow a very similar approach.&lt;/p&gt;

&lt;p&gt;We can deduce a lot from the screenshot included below. First, note the image
dimensions: 2880-by-1714. The display resolution of a 15” 2015 Retina MacBook
Pro is 2880-by-1800, so this is a screenshot of the Mac version. Also note
that at 1:1 magnification the text and the large icons are very crisp looking,
but the rest of the UI, including the render view, is made up of double-sized
(2x2) pixels—a tell-tale sign of partial Retina display support. Apparently,
they’re using native text rendering, taking advantage of high-resolution
displays whenever possible, but the rest of the UI probably consists of simple
bitmaps only. The render preview is drawn using 2x2 pixels too, most likely
due to performance reasons. Having a look at some random Windows screenshots
further validates this assumption; on those images the text looks very much
like a default 9px Tahoma UI font rendered with ClearType.&lt;/p&gt;

&lt;p&gt;It seems that similarly to REAPER, Cinema 4D uses native graphics and text
rendering to draw its UI, probably via some custom wrapper libraries.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-05-29/cinema4d.png&quot; data-width=&quot;2880&quot; data-height=&quot;1714&quot;&gt;
      &lt;img src=&quot;/files/2016-05-29/cinema4d-small.jpg&quot; alt=&quot;Cinema 4D screenshot&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;Cinema 4D on a Retina display MacBook Pro. Note that the fonts and the big icons are drawn at actual pixel resolution, while the rest of the UI is rendered with double-sized (2x2) pixels.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;nodebox3&quot;&gt;NodeBox3&lt;/h3&gt;

&lt;table class=&quot;properties&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Version&lt;/td&gt;
      &lt;td&gt;3.0.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Main executable (JAR)&lt;/td&gt;
      &lt;td&gt;22 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Libraries (ffmpeg)&lt;/td&gt;
      &lt;td&gt;12 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Java runtime&lt;/td&gt;
      &lt;td&gt;172 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Python interpreter&lt;/td&gt;
      &lt;td&gt;4.4 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Examples&lt;/td&gt;
      &lt;td&gt;8.3 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total install size&lt;/td&gt;
      &lt;td&gt;220 MiB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nodebox.net/node/&quot;&gt;NodeBox3&lt;/a&gt; is a quite interesting, open-source
generative graphics tool that runs on Windows, Mac OS X and Linux. In
contrast to the previously discussed applications, NodeBox is not a native
executable written in C++, but a Java program. The custom-looking UI is built
with the help of Swing (that’s the standard Java GUI library), as it can be
seen
&lt;a href=&quot;https://github.com/nodebox/nodebox/tree/master/src/main/java/nodebox/ui&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Relying on the cross-platform Java runtime is certainly very convenient from
the developers’ perspective, but is not without some serious drawbacks. First
of all, the whole Java runtime environment has to be bundled with the
application (at least on Windows and OS X), which accounts to a whopping 172
MiB in this concrete example (that’s 78% of the total install size!).
Interface redraws are much more sluggish compared to native applications like
REAPER and Renoise, startup times are generally slow, and Java applications
overall tend to be memory hogs. All these things are mostly non-issues for
long-running server-side applications (the most natural habitat of Java
bytecode), but they make the platform a less than ideal choice for
high-performance desktop applications.&lt;/p&gt;

&lt;p&gt;In conclusion, while Java is certainly not the  most terrible choice for
a cross-platform GUI application, it’s far from being the greatest either. For
some less demanding software (such as NodeBox) it may be an OK solution, but
I’m really averse to the idea of a DAW written in Java, where every little bit
of performance counts.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-05-29/nodebox.png&quot; data-width=&quot;1100&quot; data-height=&quot;800&quot;&gt;
      &lt;img src=&quot;/files/2016-05-29/nodebox-small.jpg&quot; alt=&quot;NodeBox3 screenshot&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;NodeBox3 has a pleasant looking GUI built using Swing that looks identical on all supported platforms, but UI redraws are noticeably sluggish even on fast computers compared to native applications that use OS level graphics APIs directly.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;light-table&quot;&gt;Light Table&lt;/h3&gt;

&lt;table class=&quot;properties&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Version&lt;/td&gt;
      &lt;td&gt;0.8.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Main executable&lt;/td&gt;
      &lt;td&gt;51 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Electron framework&lt;/td&gt;
      &lt;td&gt;42 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Resources&lt;/td&gt;
      &lt;td&gt;33 MiB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total install size&lt;/td&gt;
      &lt;td&gt;129 MiB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;http://lighttable.com/&quot;&gt;Light Table&lt;/a&gt; is a next generation code editor that
connects you to your creation with instant feedback.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This quote was taken from the project’s &lt;a href=&quot;https://github.com/LightTable/LightTable&quot;&gt;GitHub
page&lt;/a&gt; and I think it sums up this
novel IDE pretty well. Light Table is available for Windows, Mac OS X and (can
you guess?) Linux; it achieves cross-platform compatibility by leveraging the
&lt;a href=&quot;http://electron.atom.io/&quot;&gt;Electron&lt;/a&gt; framework. In basic terms, Electron
consists of a Chromium browser and Node.js, so developers can use HTML, CSS
and JavaScript to build their cross-platform desktop applications. Light Table
is actually written in ClojureScript, which transpiles to JavaScript. This makes
perfect sense, as the IDE was originally intended to be a programming
environment for Clojure only.&lt;/p&gt;

&lt;p&gt;I don’t have any personal experience using Light Table, and while I think the
application itself is a great idea, I cannot say much positive about the
underlying framework. Light Table seems to have been plagued by &lt;a href=&quot;https://github.com/LightTable/LightTable/issues/1088&quot;&gt;serious
performance issues&lt;/a&gt;
which were the main reason for them having to migrate from node-webkit (their
previous framework, now &lt;a href=&quot;http://nwjs.io/&quot;&gt;NW.js&lt;/a&gt;) to Electron at the end of
2015 (see
&lt;a href=&quot;https://m.reddit.com/r/javascript/comments/3meazr/is_electron_atom_a_good_way_to_create_offline_js/&quot;&gt;here&lt;/a&gt;).
That’s a very severe drawback of such Web-technologies-on-the-desktop style of
frameworks; they introduce too many layers of abstraction over OS provided
functionalities that often result in serious performance bottlenecks. And when
things don’t quite work as expected, you can’t do much about it—short of
maybe just switching to a different framework as a last attempt. (Of course,
this problem can happen with OS-native libraries as well, but in practice it
is much less of a problem, as OS APIs are generally several orders of
magnitude more robust and performant, and there’s much more leeway for
workarounds on the OS level if anything does not quite work as expected).&lt;/p&gt;

&lt;p&gt;Now as I think about it, the only reasons for the existence of such frameworks
is the allure of quick time to market, the luxury of being able to trade
development speed for program efficiency (because on contemporary hardware you
can be really wasteful with resources in many problem domains and the
resulting program will still remain mostly functional), and the vast armies of
newskool web developers who grew up on JavaScript and the DOM. But the
trade-offs involved are quite severe: performance, memory consumption,
installation sizes and OS integration will suffer and long-term maintenance
will be a nightmare. Despite all the drawbacks, I can see this approach work
acceptably well in some specific circumstances (non-demanding applications
aimed at a not too picky—and hopefully non-technical—audience). But if you
care about your users and cannot afford to be grossly inefficient, just stay
away from web technologies on the desktop. You don’t want to build a castle on
sand.&lt;/p&gt;

&lt;p class=&quot;warning&quot;&gt;I think of it as my duty to point out that the Electron framework carries
a terrible potential for misuse when fallen into the wrong hands. The
&lt;a href=&quot;https://github.com/maxogden/monu&quot;&gt;Monu&lt;/a&gt; OS X only process monitoring menu bar
application built using Electron weighs &lt;strong&gt;no less than 189 MiB on disk!&lt;/strong&gt; Yes,
you read that right: a heavyweight cross-platform framework featuring
a &lt;em&gt;complete built-in browser engine&lt;/em&gt; was used to create a &lt;em&gt;menu bar widget&lt;/em&gt;
for a &lt;em&gt;single platform&lt;/em&gt;!  No disrespect to the program’s author, I’m sure he
had the best intentions and he’s a nice person and all (even if he is clearly
somewhat misguided in the practical execution of his ideas), but who would
seriously entertain even just the &lt;em&gt;thought&lt;/em&gt; that a 189 MiB menu bar app was
going to be an okay thing to do, really?&lt;/p&gt;

&lt;h2 id=&quot;executive-summary&quot;&gt;Executive summary&lt;/h2&gt;

&lt;p&gt;By analysing the commonalities of the apps showcased above, we can see a few
interesting patterns emerge. For event and window handling, they all must have
used the host OS (or host platform, in the case of managed applications) in
some way, so nothing too much exciting going on there.  Regarding the
cross-platform graphics problem, each one of them implemented some kind of
variation on one of the following basic approaches:&lt;/p&gt;

&lt;ul class=&quot;compact&quot;&gt;
  &lt;li&gt;Make use of the graphics and text libraries provided by the host OS &lt;strong&gt;(1)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Use a software renderer for all graphics &lt;strong&gt;(2)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Use OpenGL for all graphics &lt;strong&gt;(3)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Use a cross-platform environment (e.g. Electron or Java) to abstract
all platform-specific stuff away &lt;strong&gt;(4)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s my &lt;em&gt;expert analysis&lt;/em&gt; on the pros and cons of each method:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;(1)&lt;/strong&gt; is probably the least pixel-identical approach across platforms
(especially text rendering can look wildly different on different operating
systems), but this is rarely a problem in practice for most applications.  In
fact, platform-native text rendering can be seen as a feature rather than
a drawback—think of Retina displays, or how hardcore Windows users typically
prefer ClearType over the unhinted OS X style text rendering, and vice versa.
While there might be some minor differences in the way different platforms
render anti-aliased vector graphics natively, those differences are generally
negligible for most use-cases (and for most users not suffering from some
chronically acute case of OCD).  Another important point to note is that OS
native graphics usually takes advantage of the GPU on most major platforms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Only &lt;strong&gt;(2)&lt;/strong&gt; guarantees to yield 100% identical results across all platforms
down to the pixel level, but it’s a lot of work and potentially it will be
slower than the often GPU-accelerated native graphics API (unless you’re
a graphics guru and know exactly what you’re doing). Overall, I think it’s
a wasted effort, unless you have some special requirements to do the rendering
in a very specific, pixel-exact way across all supported platforms. But yes,
in the past people had to come up with their own rasterizers if they were not
happy with the crappy aliased graphics provided by the Windows GDI and such.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;OpenGL &lt;strong&gt;(3)&lt;/strong&gt; initially seems to be a very good fit for cross-platform
graphics duties; after all, it’s hard to get hold of hardware nowadays (even
second-hand!) that does &lt;em&gt;not&lt;/em&gt; have support for OpenGL. But driver support can
be an issue; there’s some rumours, for example, that Intel OpenGL drivers are
pretty shit in that regard.  The other thing that can throw the spanner in the
works is font rendering.  There are lots of different approaches to it, and
some might be perfectly fine for a particular application, but generally it’s
a major pain in the ass. Just have a look at &lt;a href=&quot;http://innovation.tss-yonder.com/2012/05/14/the-future-native-cross-platform-ui-technology-that-may-not-be/&quot;&gt;this
post&lt;/a&gt;
if you don’t believe me, what kind of hoops these poor Swedes had to jump
through just to display some animated text. Oh, and you’d also need to come up
with your own tesselator engine that can construct Bézier curves out of little
triangles and so on. Compared to the native graphics approach, this is a lot
of work, more or less on par with writing your own software rasterizer. Again,
the native graphics are already doing the tesselation stuff (and lots more)
for you for free if a GPU is present, and fall back to software rendering
otherwise. But for games and similar full-screen applications where you really
want to superimpose the UI on top of the 3D view, you really don’t have much
other choice.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Option &lt;strong&gt;(4)&lt;/strong&gt; is okay (but not great) for simpler, less demanding stuff.
But when performance is a concern, this is definitely not the way to go. The
total install size will also suffers (to be very polite about this). Overall,
apart from making life easier for certain types of developers (primarily the
ones who enjoy hacking Java/Swing and the JavaScript-for-all-the-things webdev
folks), this approach has a lot of drawbacks. I also don’t think that
shoehorning web development technologies—which technically aren’t really any
better than more traditional approaches—into desktop applications is a very
sound idea either. So for any serious work, avoid.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;so-what-the-hell-to-do&quot;&gt;So what the hell to do?&lt;/h2&gt;

&lt;p&gt;…apart from writing a semi-useless blog post/rant about this utter fiasco, that
is. Looks like that an easy to use, well-performing and non-bloated library to
help with cross-platform UI and graphics in the age of Mars rovers, Go
world-champion beating AI constructs and genetically modified carrots that
will kill you in your sleep is just too much ask for.&lt;/p&gt;

&lt;p&gt;The computer says no.&lt;/p&gt;

&lt;p&gt;JUCE is probably the one that comes closest, but it’s C++, semi-bloated and
not free for commercial purposes. There’s WDL too, but that’s still C++ and
rather messy.  Also, hacking C++ in my spare time is very far from my idea of
fun. If there would be &lt;em&gt;really&lt;/em&gt; no other option, I guess I’d just stop coding
altogether and find a more relaxing hobby. Like dirt car racing, wrestling
with alligators, or disarming bombs or something.&lt;/p&gt;

&lt;p&gt;Other than JUCE (and maybe WDL), you’d have to roll your own, plain and
simple. And that’s exactly what many people have been doing, apparently, and
that’s what I’m gonna do too. To help decide how to accomplish this lofty
goal, the below table summarises my ratings of the various approaches outlined
previously. I made this really very scientific: one plus means a given feature
is somewhat enjoyable to implement, one minus means it slightly sucks, et
cetera—you get the picture.&lt;/p&gt;

&lt;p&gt;So let’s see (Java and web technologies are missing on purpose, for I really
don’t think they would be good fits for serious desktop apps):&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th style=&quot;width: 19%&quot;&gt;&lt;/th&gt;
    &lt;th style=&quot;width: 27%&quot;&gt;Software renderer&lt;/th&gt;
    &lt;th style=&quot;width: 27%&quot;&gt;Native wrapper&lt;/th&gt;
    &lt;th style=&quot;width: 27%&quot;&gt;OpenGL&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;h&quot;&gt;Windowing&lt;/td&gt;
    &lt;td&gt;
      native OS API&lt;br /&gt;
      &lt;span style=&quot;color: red; font-size: 120%; font-weight: 900&quot;&gt;-&lt;/span&gt;
    &lt;/td&gt;
    &lt;td&gt;
      native OS API&lt;br /&gt;
      &lt;span style=&quot;color: red; font-size: 120%; font-weight: 900&quot;&gt;-&lt;/span&gt;
    &lt;/td&gt;
    &lt;td&gt;
      GLFW (or similar)&lt;br /&gt;
      &lt;span style=&quot;color: green; font-size: 120%; font-weight: 900&quot;&gt;+&lt;/span&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;h&quot;&gt;Input handling&lt;/td&gt;
    &lt;td&gt;
      native input handling&lt;br /&gt;
      &lt;span style=&quot;color: red; font-size: 120%; font-weight: 900&quot;&gt;-&lt;/span&gt;
    &lt;/td&gt;
    &lt;td&gt;
      native input handling&lt;br /&gt;
      &lt;span style=&quot;color: red; font-size: 120%; font-weight: 900&quot;&gt;-&lt;/span&gt;
    &lt;/td&gt;
    &lt;td&gt;
      GLFW (or similar)&lt;br /&gt;
      &lt;span style=&quot;color: green; font-size: 120%; font-weight: 900&quot;&gt;+&lt;/span&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;h&quot;&gt;Graphics&lt;/td&gt;
    &lt;td&gt;
      custom rasterizer&lt;br /&gt;
      &lt;span style=&quot;color: red; font-size: 120%; font-weight: 900&quot;&gt;- - -&lt;/span&gt;
    &lt;/td&gt;
    &lt;td&gt;
      native graphics API&lt;br /&gt;
      &lt;span style=&quot;color: green; font-size: 120%; font-weight: 900&quot;&gt;+&lt;/span&gt;
    &lt;/td&gt;
    &lt;td&gt;
      custom tesselator&lt;br /&gt;
      &lt;span style=&quot;color: red; font-size: 120%; font-weight: 900&quot;&gt;- - -&lt;/span&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;h&quot;&gt;Text&lt;/td&gt;
    &lt;td&gt;
      FreeType&lt;br /&gt;
      &lt;span style=&quot;color: red; font-size: 120%; font-weight: 900&quot;&gt;- -&lt;/span&gt;
    &lt;/td&gt;
    &lt;td&gt;
      native text API&lt;br /&gt;
      &lt;span style=&quot;color: green; font-size: 120%; font-weight: 900&quot;&gt;+&lt;/span&gt;
    &lt;/td&gt;
    &lt;td&gt;
      FreeType (w/ font atlas)&lt;br /&gt;
      &lt;span style=&quot;color: red; font-size: 120%; font-weight: 900&quot;&gt;- -&lt;/span&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&quot;sep&quot;&gt;
    &lt;td class=&quot;h&quot;&gt;Total&lt;/td&gt;
    &lt;td&gt;&lt;span style=&quot;color: red; font-size: 120%; font-weight: 900&quot;&gt;- - - - - - -&lt;/span&gt;&lt;/td&gt;
    &lt;td&gt;&lt;span style=&quot;color: #666; font-weight: 900&quot;&gt;0&lt;/span&gt;&lt;/td&gt;
    &lt;td&gt;&lt;span style=&quot;color: red; font-size: 120%; font-weight: 900&quot;&gt;- - -&lt;/span&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The final verdict: &lt;strong&gt;the native wrapper method is the lucky winner&lt;/strong&gt; (read: it
sucks the least, compared to the major suckage involved with the other
two approaches).&lt;/p&gt;

&lt;p&gt;Phew.&lt;/p&gt;

&lt;h2 id=&quot;final-words&quot;&gt;Final words&lt;/h2&gt;

&lt;p&gt;Well, I really wanted to avoid this, but I simply must acknowledge the fact
that to produce a good quality custom cross-platform GUI in 2016, there’s
really no substitute to rolling up your sleeves and developing your own
platform-agnostic UI and graphics libraries.&lt;/p&gt;

&lt;p&gt;Oh well, I only wanted to display a few buttons and maybe push some pixels,
but fuck all that, let’s  write a whole cross-platform GUI library in Nim!&lt;/p&gt;

&lt;p&gt;Time to get serious!&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-05-29/fuck-everything.jpg&quot; alt=&quot;Fuck everything&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;The above fine
faux-leather jacket wearing gentleman already knows the secret: Qt is not
the answer to everything.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:bullshit&quot;&gt;
      &lt;p&gt;The last three new age wisdoms courtesy of the &lt;a href=&quot;http://wisdomofchopra.com/&quot;&gt;Wisdom of Chopra&lt;/a&gt; bullshit generator. &lt;a href=&quot;#fnref:bullshit&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lisp&quot;&gt;
      &lt;p&gt;Note the nested parentheses—a true tell-tale sign of my latent LISP tendencies! &lt;a href=&quot;#fnref:lisp&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
                <pubDate>Sun, 29 May 2016 00:00:00 +1000</pubDate>
                <link>http://blog.johnnovak.net/2016/05/29/cross-platform-gui-trainwreck-2016-edition/</link>
                <guid isPermaLink="true">http://blog.johnnovak.net/2016/05/29/cross-platform-gui-trainwreck-2016-edition/</guid>
            </item>
        
            <item>
                <title>The Nim Ray Tracer Project &ndash; Part 2</title>
                <description>&lt;p class=&quot;intro&quot;&gt;[I have to tell that even after 34 years, &lt;em&gt;Brian Eno&lt;/em&gt;’s ambient
music is still so much better than 99.9999% of all recent electronic releases.
Listening to &lt;em&gt;Ambient 4: On Land&lt;/em&gt; from 1982; track #2 &lt;em&gt;The Lost Day&lt;/em&gt; has
a wonderfully creepy atmosphere…]&lt;/p&gt;

&lt;h2 id=&quot;the-basics&quot;&gt;The Basics&lt;/h2&gt;

&lt;p&gt;Hello again! In the &lt;a href=&quot;2016/04/28/the-nim-raytracer-project-part1/&quot;&gt;initial
post&lt;/a&gt; of this series I talked
about ray tracing in general and my reasons for writing my own ray tracing
renderer in the most awesome &lt;a href=&quot;http://nim-lang.org/&quot;&gt;Nim&lt;/a&gt; language. Before
writing any code though, in this part we’ll examine the basic maths required
for such a renderer.&lt;/p&gt;

&lt;p&gt;Most of this stuff is a distilled version of the information contained in the
excellent &lt;a href=&quot;http://www.scratchapixel.com/&quot;&gt;Scratchapixel 2.0 – Learn Computer Graphics Programming from
Scratch&lt;/a&gt; learning resource, which I highly
recommend to all graphics programming enthusiasts. There’s absolutely zero
point in recreating the superb explanations from those lessons here, so just
go and read the original materials if you’re interested.&lt;/p&gt;

&lt;p&gt;My only problem with Scratchapixel was that while the content is generally of
very high quality, the mathematical notation they use is sometimes a bit
sloppy and a few formulas are actually incorrect (just a &lt;em&gt;very few&lt;/em&gt;, to be
fair). I tried to supplement Scratchapixel with other random learning
materials found on the Internet but that also proved to be problematic because
of the slightly different notational conventions and assumptions of different
authors (e.g. some people assume a right-handed while others a left-handed
coordinate system, maths folks like their z-axis to point upwards, but in
computer graphics that’s usually the y-axis and the z-axis points towards the
viewer, then there’s the difference between row and column vector notation and
so on).  While these differences might be laughably trivial to a mathematician
or to someone who is already pretty familiar with the subject, they can surely
confuse the hell out of a newcomer, like myself (or maybe I just get confused
too easily, that might very well be the case…).&lt;/p&gt;

&lt;p&gt;I’m not exactly the biggest fan of littering the source code with page-long
explanatory comments  either, so all my development notes will end up in these
blog posts. That way I will have at least a remote chance of understanding
what the hell I was doing here a couple of years down the track, and maybe
others will find my development diary also useful (or amusing, depending on
their perspective…).&lt;/p&gt;

&lt;p&gt;Alright, let’s get down to business!&lt;/p&gt;

&lt;h3 id=&quot;coordinate-system&quot;&gt;Coordinate system&lt;/h3&gt;

&lt;p&gt;We are going to use a &lt;a href=&quot;https://en.wikipedia.org/wiki/Cartesian_coordinate_system#In_three_dimensions&quot;&gt;right-handed Cartesian coordinate system
&lt;/a&gt;
to represents objects in our 3D world, where the &lt;em&gt;y-axis&lt;/em&gt; points up, the
&lt;em&gt;x-axis&lt;/em&gt; to the right and the &lt;em&gt;z-axis&lt;/em&gt; “outward”. In right-handed coordinate
systems, positive rotation is
&lt;a href=&quot;https://www.evl.uic.edu/ralph/508S98/coordinates.html&quot;&gt;counter-clockwise&lt;/a&gt;
about the axis of rotation.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-04-30/coordinate-system.svg&quot; alt=&quot;Figure 1 &amp;mdash; The right-handed coordinate system used in our renderer.&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; text-align: center;&quot;&gt;Figure 1 &amp;mdash; The right-handed coordinate system used in our renderer. The circular arrow indicates the direction of positive rotation.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;The choice of coordinate system handedness is nothing more than a convention:
DirectX, Unity, Maya and Pixar’s RenderMan use left-handed coordinate systems,
while OpenGL, &lt;a href=&quot;http://www.pbrt.org/&quot;&gt;pbrt&lt;/a&gt; and most other 3D modelling
software are right-handed. For our purposes, compatibility with OpenGL and
pbrt are the most important. Also, right-handed coordinate systems are the
norm in mathematics, which will also make life a bit easier.&lt;/p&gt;

&lt;h3 id=&quot;notation&quot;&gt;Notation&lt;/h3&gt;

&lt;p&gt;As I mentioned, the aim is to use a consistent mathematical notation throughout
the whole series, so let’s define that first!&lt;/p&gt;

&lt;p&gt;$$\cl&quot;ma-legend-align&quot;{\table
f, \text&quot;scalar&quot;;
P, \text&quot;point&quot;;
(P_x, P_y, P_z), \text&quot;point (by coordinates)&quot;;
\AB, \text&quot;segment&quot;;
v↖{→}, \text&quot;vector&quot;;
⟨\v_\x, \v_\y, \v_\z⟩, \text&quot;vector (by coordinates)&quot;;
n↖{∧}, \text&quot;unit vector)&quot;;
{‖v↖{→}‖}, \text&quot;magnitude (length) of vector&quot;;
a↖{→}·b↖{→}, \text&quot;dot product&quot;;
a↖{→}×b↖{→}, \text&quot;cross product&quot;;
\bo M, \text&quot;matrix&quot;;
}$$&lt;/p&gt;

&lt;p&gt;Column notation is used for vectors:&lt;/p&gt;

&lt;p&gt;$$ v↖{→}=[\table x; y; z; w; ] $$&lt;/p&gt;

&lt;p class=&quot;note&quot;&gt;If the formulas look like crap in your browser, that means it
sadly doesn’t support MathML. Solution? Use a better browser, like
&lt;a href=&quot;https://www.mozilla.org/en-US/firefox/new/&quot;&gt;Firefox&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;transform-matrices&quot;&gt;Transform matrices&lt;/h3&gt;

&lt;p&gt;All the matrix stuff will be eventually handled by a &lt;a href=&quot;https://github.com/stavenko/nim-glm&quot;&gt;matrix
library&lt;/a&gt; written by someone else, but
it’s still useful to know how these transforms look like in our right-handed
coordinate system (for example, for performance purposes we might hand-code
some optimised versions of these transforms later).&lt;/p&gt;

&lt;h4 id=&quot;translation-and-scaling&quot;&gt;Translation and scaling&lt;/h4&gt;

&lt;p&gt;$$\bo T=[\table
1, 0, 0, t_x;
0, 1, 0, t_y;
0, 0, 1, t_z;
0, 0, 0, 1;
]
\;\;\;\;\;\;\bo S=[\table
s_x,   0,   0, 0;
  0, s_y,   0, 0;
  0,   0, s_z, 0;
  0,   0,   0, 1;
]$$&lt;/p&gt;

&lt;h4 id=&quot;rotation-around-a-given-axis&quot;&gt;Rotation around a given axis&lt;/h4&gt;

&lt;p&gt;$$\bo R_\x=[\table
1,      0,       0, 0;
0, \cos θ, -\sin θ, 0;
0, \sin θ,  \cos θ, 0;
0,      0,       0, 1;
]
\;\;\;\;\;\;\bo R_\y=[\table
 \cos θ, 0, \sin θ, 0;
      0, 1,      0, 0;
-\sin θ, 0, \cos θ, 0;
      0, 0,      0, 1;
]$$&lt;/p&gt;

&lt;p&gt;$$\bo R_\z=[\table
\cos θ, -\sin θ, 0, 0;
\sin θ,  \cos θ, 0, 0;
     0,       0, 1, 0;
     0,       0, 0, 1;
]$$&lt;/p&gt;

&lt;h3 id=&quot;calculating-primary-rays&quot;&gt;Calculating primary rays&lt;/h3&gt;

&lt;p&gt;Let $(P_x, P_y)$ be the &lt;strong&gt;pixel coordinates&lt;/strong&gt; of a pixel of the final image,
$w$ and $h$ the width and the height of the image in pixels and $r = w / h$
the image aspect ratio.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-04-30/mappings.svg&quot; alt=&quot;Figure 2 &amp;mdash; The relationships between the raster, NDC and screen spaces.&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%; text-align: center;&quot;&gt;Figure 2 &amp;mdash; The relationships between the raster, NDC and screen spaces.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;We have to shoot the rays through the middle of the pixels, thus the $(R_x,
R_y)$ &lt;strong&gt;raster coordinates&lt;/strong&gt; of a given pixel are as follows:&lt;/p&gt;

&lt;p&gt;$$\cl&quot;ma-join-align&quot;{\table
R_x ,= P_x + 0.5;
R_y ,= P_y + 0.5;
}$$&lt;/p&gt;

&lt;p&gt;From this follows the formula for calculating the $(N_x, N_y)$
&lt;strong&gt;normalised device coordinates (NDC)&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$\cl&quot;ma-join-align&quot;{\table
N_x ,= R_x / w r;
N_y ,= R_y / h;
}$$&lt;/p&gt;

&lt;p&gt;And finally the $(S_x, S_y)$ &lt;strong&gt;screen coordinates&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$\cl&quot;ma-join-align&quot;{\table
S_x ,= 2 N_x - r;
S_y ,= -(2 N_y - 1)
}$$&lt;/p&gt;

&lt;p&gt;To simplify the calculations, let the image plane be -1 distance away from the
origin on the z-axis and let $α$ be the &lt;strong&gt;vertical field of view
(FOV)&lt;/strong&gt; of the camera. From Figure 3 it can be seen that by default the field
of view is 90° (because $\tan 90° / 2 = \tan 45° = 1 = \BC $), and
the length of $\BC$ is actually the $f$ &lt;strong&gt;field of view factor&lt;/strong&gt; of the
camera.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-04-30/fov.svg&quot; alt=&quot;Figure 3 &amp;mdash; Calculating the vertical field of view (FOV) factor.&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%; text-align: center;&quot;&gt;Figure 3 &amp;mdash; Calculating the vertical field of view (FOV) factor.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;$$\tan α / 2 = \BC / \AB = \BC / 1$$
$$\BC = \tan α / 2 = f$$&lt;/p&gt;

&lt;p&gt;To obtain the desired field of view, the image surface has to be scaled by
the field of view factor (this is akin to zooming with a traditional camera
lens). Thus we yield the screen coordinates normalised by the field
of view factor:&lt;/p&gt;

&lt;p&gt;$$\cl&quot;ma-join-align&quot;{\table
S_x ,= (2 N_x - r) f;
S_y ,= -(2 N_y - 1) f;
}$$&lt;/p&gt;

&lt;p&gt;After substitutions, the final transform from pixel coordinates to screen
coordinates looks like this:&lt;/p&gt;

&lt;p&gt;$$\cl&quot;ma-join-align&quot;{\table
S_x ,= ({2 (P_x + 0.5) r} / w - r) f;
S_y ,= (1 - {2 (P_y + 0.5)} / h) f;
}$$&lt;/p&gt;

&lt;p&gt;So for each pixel $(P_x, P_y)$ in the image we can now calculate the
corresponding screen coordinates $(S_x, S_y)$ we’ll need to shoot the
primary rays through. Since the camera is at the origin, the direction vector
$d↖{∧}$ of the ray corresponding to pixel $(P_x, P_y)$ is simply the vector
$⟨S_x, S_y⟩$ normalised:&lt;/p&gt;

&lt;p&gt;$$ d↖{∧} = ⟨S_x, S_y⟩ / {‖⟨S_x, S_y⟩‖}$$&lt;/p&gt;

&lt;p&gt;As the last step, we’ll need to multiply the resulting direction vector
$d↖{∧}$ and the camera position $O$ (which is at the origin by default) with
the $\bo C$ camera-to-world transform matrix:&lt;/p&gt;

&lt;p&gt;$$ d_{\w}↖{∧} = \bo C d↖{∧}$$
$$ O_{\w} = \bo C O$$&lt;/p&gt;

&lt;p&gt;Note that assuming 4-component vectors and a 4x4 transform matrix that is used
for both translation and rotation, the above will only work correctly if the
fourth $w$ component of the direction vector $d↖{∧}$ is set to 0, and the $w$
component of the point $O$ is set to 1. Remember that the camera position is
at the origin before the transform, so $O$ will be always equal to this:&lt;/p&gt;

&lt;p&gt;$$ O=[\table 0; 0; 0; 1; ] $$&lt;/p&gt;

&lt;h3 id=&quot;ray-sphere-intersection&quot;&gt;Ray-sphere intersection&lt;/h3&gt;

&lt;p&gt;The implicit equation of a &lt;strong&gt;sphere&lt;/strong&gt; with centre point $C$ and radius $r$:&lt;/p&gt;

&lt;p&gt;$$(x-C_x)^2 + (y-C_y)^2 + (z-C_z)^2 = r^2$$&lt;/p&gt;

&lt;p&gt;The parametric equation of a &lt;strong&gt;half-open line segment&lt;/strong&gt; (the ray, in our
case), where $\O$ is the starting point, $d↖{∧}$ the direction vector and $P$
a point on the segment for any $t≧0$:&lt;/p&gt;

&lt;p&gt;$$P = O + d↖{∧}t$$&lt;/p&gt;

&lt;p&gt;Written component-wise:&lt;/p&gt;

&lt;p&gt;$$P_x = O_x + d_xt $$
$$P_y = O_y + d_yt $$
$$P_z = O_z + d_zt $$&lt;/p&gt;

&lt;p&gt;To get the ray-sphere intersection points, we’ll need to substitute $P_x$,
$P_y$ and $P_z$ into the equation of the sphere:&lt;/p&gt;

&lt;p&gt;$$(O_x + d_xt - C_x)^2 + (O_y + d_yt-C_y)^2 + (O_z + d_zt-C_z)^2 = r^2$$&lt;/p&gt;

&lt;p&gt;The first parenthesised expression can be expanded like this:&lt;/p&gt;

&lt;p&gt;$$\cl&quot;ma-join-align&quot;{\table
(O_x + d_xt - C_x)^2 ,= O_x^2 + O_x d_xt - O_xC_x + d_xtO_x + d_x^2t^2 - d_xtC_x - C_xO_x - C_xd_xt + C_x^2;
 ,= O_x^2 + 2O_x d_xt - 2O_xC_x + d_x^2t^2 - 2C_x d_xt + C_x^2;
 ,= d_x^2t^2 + (2O_x d_x - 2C_x d_x)t + (O_x^2 - 2O_xC_x + C_x^2);
 ,= d_x^2t^2 + (2d_x(O_x- C_x))t + (O_x - C_x)^2;
}$$&lt;/p&gt;

&lt;p&gt;The remaining two expressions can be expanded in a similar way, so the final
equation will have the form of a quadratic equation $ at^2 + bt + c = 0$,
where:&lt;/p&gt;

&lt;p&gt;$$\cl&quot;ma-join-align&quot;{\table
a ,= d_x^2 + d_y^2 + d_z^2;
b ,= 2 (d_x(O_x - C_x) + d_y(O_y - C_y) + d_z(O_z - C_z));
c ,= (O_x - C_x)^2 + (O_y - C_y)^2 + (O_z - C_z)^2;
}$$&lt;/p&gt;

&lt;p&gt;First the discriminant $\Δ$ needs to be calculated. If $\Δ &amp;lt; 0$, the
ray does not intersect the sphere; if $\Δ = 0$, the ray touches the sphere
(one intersection point); and if $\Δ &amp;gt; 0$, it intersects the sphere at
two points. The equation can be solved for $t$ by applying the following
formula that takes care of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Loss_of_significance#A_better_algorithm&quot;&gt;loss of
significance&lt;/a&gt;
floating-point problem:&lt;/p&gt;

&lt;p&gt;$$\Δ = b^2-4ac$$&lt;/p&gt;

&lt;p&gt;$$t_1 = {-b-\sgn(b)√\Δ} / {4a}\;\;\;,\;\;\;t_2 = c/{a t_1}$$&lt;/p&gt;

&lt;h3 id=&quot;ray-plane-intersection&quot;&gt;Ray-plane intersection&lt;/h3&gt;

&lt;p&gt;A &lt;strong&gt;plane&lt;/strong&gt; can be defined by a normal vector $n↖{∧}$ and a point $P_o$,
where $n↖{∧}$ represents the orientation of the plane and $P_o$ how far away
the plane is from the origin. We know that the dot product of two vectors is
zero only if they are perpendicular to each other, from which follows that for
every point $\P$ that lies on the plane defined by $n↖{∧}$ and $P_o$ the
following holds true:&lt;/p&gt;

&lt;p&gt;$$(P-P_o)·n↖{∧}=0$$&lt;/p&gt;

&lt;p&gt;To get the ray-plane intersection point, we only need to substitute the
parametric equation of the ray $\P = \O + d↖{∧}t$ into the above equation and
solve it for $t$ :&lt;/p&gt;

&lt;p&gt;$$(O + d↖{∧}t-P_o)·n↖{∧}=0$$
$$d↖{∧}t·n↖{∧} + (O -P_o)·n↖{∧}=0$$
$$t = {(P_o - O)·n↖{∧}} / {n↖{∧}·d↖{∧}}$$&lt;/p&gt;

&lt;p&gt;As usual, we are interested in positive $t$ values only. If $t$ is zero (or
very close to zero), then there is no intersection because the ray is parallel
with the plane, either away from it or exactly coinciding with it.&lt;/p&gt;

&lt;h2 id=&quot;in-the-next-episode&quot;&gt;In the next episode…&lt;/h2&gt;

&lt;p&gt;Believe it or not, that’s all we maths we need to implement a very simple
ray tracer capable of rendering planes and spheres! In the next part, we’ll
inspect some actual Nim code that generated this singular masterpiece of 80’s
CGI art below. (Well, you gotta start somewhere, right?)&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-04-30/lame-first-render.jpg&quot; data-width=&quot;1200&quot; data-height=&quot;800&quot;&gt;
      &lt;img src=&quot;/files/2016-04-30/lame-first-render.jpg&quot; alt=&quot;My lame first render&quot; style=&quot;width: 80%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 80%;&quot;&gt;This might look like total crap, but it&#39;s 16x MSAA anti-aliased, yo, and it was generated by first ray tracing &quot;engine&quot;!&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;hr /&gt;

&lt;section class=&quot;links&quot;&gt;

  &lt;h2 id=&quot;suggested-reading&quot;&gt;Suggested reading&lt;/h2&gt;

  &lt;ul class=&quot;compact&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.scratchapixel.com/&quot;&gt;Scratchapixel 2.0 – Learn Computer Graphics Programming from
Scratch&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

&lt;/section&gt;
</description>
                <pubDate>Sat, 30 Apr 2016 00:00:00 +1000</pubDate>
                <link>http://blog.johnnovak.net/2016/04/30/the-nim-raytracer-project-part2/</link>
                <guid isPermaLink="true">http://blog.johnnovak.net/2016/04/30/the-nim-raytracer-project-part2/</guid>
            </item>
        
            <item>
                <title>The Nim Ray Tracer Project &ndash; Part 1</title>
                <description>&lt;p class=&quot;intro&quot;&gt;[Listening to the albums &lt;em&gt;Reverberant Skies&lt;/em&gt; and &lt;em&gt;White Maps&lt;/em&gt; from
&lt;em&gt;Aglaia&lt;/em&gt; in an infinite loop… A beautifully hypnotic experience! The track
&lt;em&gt;In the First Spinning Place&lt;/em&gt; from &lt;em&gt;White Maps&lt;/em&gt; is my absolute favourite.]&lt;/p&gt;

&lt;h2 id=&quot;why-a-ray-tracer&quot;&gt;Why a ray tracer?&lt;/h2&gt;

&lt;p&gt;I’ve been fascinated by ray tracing since my childhood. I still remember the
first time I was playing around with an early 3D modeller/ray tracer program
on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Amiga_500&quot;&gt;Amiga 500&lt;/a&gt; called
&lt;a href=&quot;https://en.wikipedia.org/wiki/Imagine_%283D_modeling_software%29&quot;&gt;Imagine&lt;/a&gt;
when I was about 13, trying to render some extruded letters made of glass
placed on a classic checker-board patterned surface (don’t laugh, this was
sort of a novel thing to do at that time, and I was a kid). Well, rendering
the final 360x576 image (&lt;a href=&quot;https://en.wikipedia.org/wiki/Overscan&amp;gt;&quot;&gt;overscan&lt;/a&gt;!)
took a bit more than 7 hours, but when I turned on the monitor in the morning
in great anticipation and saw the final image in its
&lt;a href=&quot;https://en.wikipedia.org/wiki/Hold-And-Modify&quot;&gt;4096-colour&lt;/a&gt; interlaced glory
(probably quite primitive looking by today’s standards), I was in awe! What
made the experience even more interesting for me was that Imagine supported
the use of so-called procedural textures, which are textures solely generated
by mathematical functions instead of using bitmap images. I was mesmerised—a
photorealistic image generated by the computer using nothing else but pure
maths! &lt;sup id=&quot;fnref:amiga&quot;&gt;&lt;a href=&quot;#fn:amiga&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;I have always thought of ray tracing as some kind of black magic—something
utterly fascinating and very scary at the same time because of the complex
maths involved (which, as it turns out, is not quite true). This belief was
also strengthened by my modest excursions into OpenGL programming years later,
which uses a different method called
&lt;a href=&quot;https://en.wikipedia.org/wiki/Rasterisation&quot;&gt;rasterization&lt;/a&gt; to generate 3D
images.&lt;/p&gt;

&lt;h2 id=&quot;ray-tracing-vs-rasterization&quot;&gt;Ray tracing vs rasterization&lt;/h2&gt;

&lt;p&gt;Historically, there have been two main disparate approaches to rendering 3D
scenes, rasterization and ray tracing (the situation is not so clear-cut
nowadays, as we’ll see later). While rasterization is by several orders of
magnitude more efficient at producing 3D animations at smooth frame rates in
real-time, ray tracing can produce vastly more photorealistic results.  While
many visual effects that happen in real life, such as non-planar reflections,
soft shadows, refractions and caustics, are quite simple, albeit
computationally very costly, to calculate with ray tracing, it requires quite
a bit of complicated trickery even just to fake them with rasterization.&lt;/p&gt;

&lt;p&gt;At the risk of grossly oversimplifying matters, rasterization is very
efficient at projecting several hundreds of thousands of three-dimensional
triangles onto a two-dimensional surface (the screen) and then colouring
(shading) them according to some rules. In order to obtain reasonable frame
rates during real-time animation, certain simplifications and optimisations
have to be made. Photorealistic rendering that accurately portrays how a given
scene would look in real life is not necessarily of primary importance as long
as the end result looks pleasing (which is a reasonable assumption for many
applications such as visualisation and games, where absolute fidelity to
reality is not a requirement). It’s the 3D artists’ job to arrange those
coloured triangles so that the resulting image looks good. Most
graphics-accelerator cards today implement some sort of rasterization pipeline
in hardware.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_tracing_(graphics)&quot;&gt;Ray tracing&lt;/a&gt;, on the
other hand, is a pretty much a no-holds-barred approach of generating
realistic images on a computer by simulating the path of photons emitted by
light sources bouncing from surface to surface among the objects making
up the 3D scene, finally landing on the image surface of the virtual camera.
Photorealism is of primary importance here, which is achieved by calculating
the image pixel by pixel using algorithms that mimic the laws of physics as
closely as practically possible (certain simplifications must be made, of
course, otherwise we would end up writing a Universe Simulator!).&lt;/p&gt;

&lt;p&gt;This begs the question, is ray tracing superior to rasterization then? In some
way, we can say it is.  As far as photorealistic rendering is concerned, ray
tracing is hard to beat, however this comes at a very steep computational
cost. But if we asked whether it was possible to create pleasing imagery using
rasterization alone, the answer would be a yes again. Just because
rasterization is not as much rooted in physical reality as ray tracing, it
would be foolish to think that it’s not capable of producing stellar results
that can look very convincingly real (just look at any modern game released
post 2010!). Indeed, there’s nothing preventing skilful artists from arranging
the triangles making up the scene in such a way that can convey very realistic
final results—similarly to how a master painter can create astonishingly
realistic works of art by nothing more than applying differently coloured
specks of paint onto a canvas with a mere paintbrush.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-04-28/caravaggio.jpg&quot; data-width=&quot;800&quot; data-height=&quot;822&quot;&gt;
      &lt;img src=&quot;/files/2016-04-28/caravaggio.jpg&quot; alt=&quot;Caravaggio, The Calling of St Matthew&quot; style=&quot;width: 80%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 80%;&quot;&gt;Surely, &lt;a href=&quot;https://en.wikipedia.org/wiki/Caravaggio&quot;&gt;Caravaggio&lt;/a&gt; did not know about the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fresnel_equations&quot;&gt;Fresnel equations&lt;/a&gt; or the &lt;a href=&quot;https://en.wikipedia.org/wiki/Metropolis_light_transport&quot;&gt;Metropolis light transport&lt;/a&gt; when he painted &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Calling_of_St_Matthew_(Caravaggio)&quot;&gt;The Calling of Saint Matthew&lt;/a&gt; in 1600. Yet no one would say his paintings are not realistic enough just because he did not know about the physics of light! Interestingly enough, there have been some &lt;a href=&quot;http://www.webexhibits.org/hockneyoptics/post/grundy7.html&quot;&gt;speculations&lt;/a&gt; that he might have used the &lt;a href=&quot;https://en.wikipedia.org/wiki/Camera_obscura&quot;&gt;camera obscura&lt;/a&gt; to aid him in attaining his photorealistic results, the workings of which have strong connections to the basic idea of ray tracing. Here we go!&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;As CPUs and GPUs grew more powerful, previously purely
rasterization-based game engines have been increasingly incorporating ray
tracing techniques into their toolbox, such as &lt;a href=&quot;http://blogs.unity3d.com/2014/09/18/global-illumination-in-unity-5/&quot;&gt;global
illumination&lt;/a&gt;,
&lt;a href=&quot;http://twiik.net/articles/realtime-reflections-in-unity-5&quot;&gt;real-time
reflections&lt;/a&gt; and
&lt;a href=&quot;https://vimeo.com/82659909&quot;&gt;pseudo-refraction&lt;/a&gt; using shaders. These techniques
are usually quite limited compared to their ray tracing counterparts, for
example global illumination is usually restricted to static objects,
reflections are only possible on planar surfaces and refraction is faked with
shaders.&lt;/p&gt;

&lt;p&gt;Also, more and more game engines have started implementing &lt;a href=&quot;http://blog.wolfire.com/2015/10/Physically-based-rendering&quot;&gt;physically-based
rendering (PBR)&lt;/a&gt;
methods recently, the renowned
&lt;a href=&quot;https://en.wikipedia.org/wiki/CryEngine&quot;&gt;CryEngine&lt;/a&gt; being one notable
example. The following is the introductory paragraph of the &lt;a href=&quot;http://docs.cryengine.com/display/SDKDOC2/Physically+Based+Shading&quot;&gt;Physically Based
Shading&lt;/a&gt;
chapter of the &lt;a href=&quot;http://docs.cryengine.com/display/SDKDOC1/Home&quot;&gt;CryEngine SDK
documentation&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;CRYENGINE 3.6 and beyond uses a shading model that is based on fundamental
  physical rules. Instead of using a plenty of fudge and tweak factors which
  don’t have a reasonable meaning in the real world, materials use some physical
  properties to describe how the incoming light should interact with them.
  A huge advantage of using a physically based model with certain rules is that
  material assets will a lot more consistent and look more convincing under
  different lighting conditions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This supports my earlier claim that while it’s certainly possible to create
realistic visuals with rasterization, photorealism is not inherent in the
rendering algorithm, but requires manual tweaking and fine-tuning of various
parameters on the part of the 3D artist.&lt;/p&gt;

&lt;p&gt;To sum up, I personally tend to think of rasterization as a more
artist-centric and ray tracing as a more scientific approach of image
generation. Ultimately, both are just tools of solving the problem of mapping
a 3D scene onto a 2D image surface, but there’s a certain undeniable beauty
and elegance to ray tracing algorithms in how they generate complex visual and
optical effects by the application of just a few simplified physical models of
reality.&lt;/p&gt;

&lt;h3 id=&quot;in-defence-of-rasterization&quot;&gt;In defence of rasterization&lt;/h3&gt;

&lt;p&gt;Just to bring the point home that rasterization is not inferior to ray tracing
but only different, and to further support my argument that it is a more
artist-based approach to rendering, let me present two examples from the two
opposite ends of the spectrum of what rasterization is capable of. As we’ll
see, it would have been difficult or impractical (or both) to achieve the same
results with pure ray tracing techniques alone in both cases.&lt;/p&gt;

&lt;p&gt;The magical realist adventure game (whatever that means) &lt;a href=&quot;http://kentuckyroutezero.com/&quot;&gt;Kentucky Road
Zero&lt;/a&gt; by the aptly titled developer &lt;a href=&quot;http://cardboardcomputer.com/&quot;&gt;Cardboard
Computer&lt;/a&gt; uses clever rasterization tricks to
render its highly stylised low-polygon world. While the abstract visuals bear
similitude to simple 2D paper cut-outs in most scenes, some occasional camera
angle switches hint at it that there’s more going on under the hood, and
indeed, this is in fact the 3D engine &lt;a href=&quot;https://unity3d.com/&quot;&gt;Unity&lt;/a&gt; in action,
as evidenced by this &lt;a href=&quot;https://unity3d.com/showcase/case-stories/cardboardcomputer-kentuckyroutezero&quot;&gt;featured
article&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-04-28/krz.jpg&quot; data-width=&quot;1920&quot; data-height=&quot;1080&quot;&gt;
      &lt;img src=&quot;/files/2016-04-28/krz.jpg&quot; alt=&quot;Kentucky Road Zero, Act I in-game screenshot&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;The opening scene of &lt;a href=&quot;http://kentuckyroutezero.com/&quot;&gt;Kentucky Road Zero&lt;/a&gt;, Act I by &lt;a href=&quot;http://cardboardcomputer.com/&quot;&gt;Cardboard Computer&lt;/a&gt;. If David Lynch ever ventured into making a computer game, then this would be it. The unique visual style of the game proves that an unconventional, abstract approach to rendering instead of taking the well-beaten &quot;standard&quot; photorealistic path can yield much more interesting results.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;&lt;a href=&quot;http://www.theastronauts.com/&quot;&gt;The Astronauts&lt;/a&gt;’ first-person mystery game
&lt;a href=&quot;http://ethancartergame.com/&quot;&gt;The Vanishing of Ethan Carter&lt;/a&gt;, on the other
hand, employs a breathtakingly beautiful painterly approach to create its
immersive in-game atmosphere. As it can be clearly seen on the screenshot, the
results are stunningly realistic, but in a dreamy and artistic kind of way
which is not dissimilar at all to the Caravaggio painting presented above. The
developers used a technique called
&lt;a href=&quot;http://www.theastronauts.com/2014/03/visual-revolution-vanishing-ethan-carter/&quot;&gt;photogrammetery&lt;/a&gt;
to effectively sample reality in the form of textures and 3D meshes from
thousands of photographs, but from there on it’s all traditional rasterization
using the &lt;a href=&quot;https://www.unrealengine.com/&quot;&gt;Unreal Engine&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
    &lt;a class=&quot;largeimg&quot; href=&quot;/files/2016-04-28/ethan-carter.jpg&quot; data-width=&quot;1920&quot; data-height=&quot;1080&quot;&gt;
      &lt;img src=&quot;/files/2016-04-28/ethan-carter.jpg&quot; alt=&quot;The Vanishing of Ethan Carter in-game screenshot&quot; style=&quot;width: 100%&quot; /&gt;
    &lt;/a&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;One of the most beautiful games of recent years, &lt;a href=&quot;http://ethancartergame.com/&quot;&gt;The Vanishing of Ethan Carter&lt;/a&gt; pulls the player in into its highly realistic yet painterly world right from the first second of gameplay. No trace (pun intended) of physical based rendering is to be found here, yet the results speak for themselves.&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;I suspect it would have been quite difficult to achieve the same level of
painterly feel by employing a strict ray tracing approach, even if that was
possible at fluid frame rates on current hardware at all.  While ray tracing
would have certainly yielded a more faithful portrayal of reality in terms of
sheer physical accuracy, it could not have granted the artist as much creative
freedom to sculpt the games’s almost otherwordly beautiful scenes by freely
manipulating the shades and colours.  Atmosphere and emotions beat pure
physics in this example again.&lt;/p&gt;

&lt;h2 id=&quot;why-nim&quot;&gt;Why Nim?&lt;/h2&gt;

&lt;p&gt;You know the old saying, mastering a programming language just by reading
about it is like trying to learn how to ride the bicycle from a book.  I get
quickly bored by solving toy textbook exercises too; implementing quicksort in
yet another language is not quite my idea of having a good time, really.  What
works for me best is writing a new application from scratch that does some
cool stuff that I’m excited about and learn the new language along the way. So
that’s what we’re gonna do here, write a full-featured ray tracer from the
ground up in pure Nim!&lt;/p&gt;

&lt;p&gt;Oh, and why &lt;a href=&quot;http://nim-lang.org/&quot;&gt;Nim&lt;/a&gt;? Serious people™ use C++ for
high-performance graphics stuff, don’t they? Well, this is going to be a ray
tracer, so speed matters &lt;em&gt;a lot&lt;/em&gt;, indeed—but so does the fun factor and my
sanity too, as I’m not paid to suffer here, this being a hobby project and all
(on a related note, don’t be a masochist, &lt;a href=&quot;/2016/03/03/the-quest-for-a-programming-language-that-doesnt-suck-part1/&quot;&gt;just say No to
C++!&lt;/a&gt;).
Nim claims to be “Efficient like C, expressive like Python and flexible like
Lisp” which sounds pretty good in my book. In fact, C, Python and Lisp are
probably my three all-time favourite languages, so this actually sounds more
like a dream come true! So what we’re gonna do here is take this little guy
for a wild ride and see how it lives up to its lofty goals in the real world!&lt;/p&gt;

&lt;h2 id=&quot;in-the-next-episode&quot;&gt;In the next episode…&lt;/h2&gt;

&lt;p&gt;We will examine the mathematical foundations of writing a simple ray tracer.
Time to brush up those high-school trigonometry skills! Stay tuned!&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:amiga&quot;&gt;
      &lt;p&gt;To put things into perspective, my experimentations with ray tracing on the Amiga took place in around 1992. There was virtually no Internet back then, I haven’t even heard the word until several years later. I read it in a &lt;em&gt;paper computer magazine&lt;/em&gt; (yes, those things &lt;em&gt;did&lt;/em&gt; actually exist, can you believe it?) that the CGI effects in the 1991 film &lt;a href=&quot;https://en.wikipedia.org/wiki/Terminator_2:_Judgment_Day&quot;&gt;Terminator 2: Judgement Day&lt;/a&gt; were rendered on a room full of ultra high-end (and ultra-expensive) &lt;a href=&quot;http://www.obsolyte.com/sgi_iris/&quot;&gt;Silicon Graphics IRIS&lt;/a&gt; &lt;a href=&quot;http://www.sgistuff.net/funstuff/hollywood/&quot;&gt;workstations&lt;/a&gt;, but even with that kind of computing power it took &lt;em&gt;several months&lt;/em&gt; to render a few minutes’ worth of scenes. Now, seeing &lt;em&gt;any kind of&lt;/em&gt; semi-photorealistic ray traced image come out of my modest &lt;a href=&quot;https://en.wikipedia.org/wiki/Amiga_500&quot;&gt;Amiga 500&lt;/a&gt; personal computer setup sporting a blazingly fast 7.14 Mhz &lt;a href=&quot;https://en.wikipedia.org/wiki/Motorola_68000&quot;&gt;Motorola 68000 CPU&lt;/a&gt; seemed nothing short of a small miracle to me then! (Actually, that chip is as old as &lt;em&gt;yours truly&lt;/em&gt;, I’ve just checked…) Oh, and I had no such luxuries as a &lt;em&gt;hard disk drive&lt;/em&gt; either—the whole program and the scene to be rendered had to fit into the machine’s whopping 1 MB of RAM (expanded from the 512 KB factory default) and the final image was then slowly written to a blank &lt;a href=&quot;https://en.wikipedia.org/wiki/Floppy_disk_variants#Commodore_Amiga&quot;&gt;880 KB floppy disk&lt;/a&gt; over the course of 7-8 hours! Eh, kids these days with their GPU-accelerated Monte Carlo path tracers… &lt;a href=&quot;#fnref:amiga&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
                <pubDate>Thu, 28 Apr 2016 00:00:00 +1000</pubDate>
                <link>http://blog.johnnovak.net/2016/04/28/the-nim-raytracer-project-part1/</link>
                <guid isPermaLink="true">http://blog.johnnovak.net/2016/04/28/the-nim-raytracer-project-part1/</guid>
            </item>
        
            <item>
                <title>The Quest for a Programming Language That Doesn't Suck &ndash; Part 1</title>
                <description>&lt;p class=&quot;intro&quot;&gt;[Listening to: &lt;em&gt;Aglaia – Three Organic Experiences&lt;/em&gt;. Track #3, &lt;em&gt;Seven
Ancient Glaciers&lt;/em&gt; is a definite stand-out!]&lt;/p&gt;

&lt;p class=&quot;intro&quot;&gt;Welcome, stranger! This series is a journal that attempts to document my
trials and tribulations as I embark on the impossible journey of finding the
perfect programming language that I can use with great joy on my hobby
projects. An epic quest of unfathomable difficulty, indeed!&lt;/p&gt;

&lt;p class=&quot;intro&quot;&gt;In this episode: after a somewhat lacklustre introductory meandering on the
merits of the multi-language approach in software development, things get
heated up quite a bit with some good old-fashioned C++ bashing, after which
the hero calms down and finally sets some GOALs. A rare Greek mythological
creature also makes an appearance midway.&lt;/p&gt;

&lt;h2 id=&quot;performance-vs-productivity&quot;&gt;Performance vs productivity&lt;/h2&gt;

&lt;p&gt;A good programmer is a good programmer in any language. Yet the choice of
language has significant effects on both developer productivity and the
runtime performance of the resulting software. The “universally accepted
truth” appears to be that these two things form a sort of a dichotomy.
Low-level languages allow ultimate control over the hardware, thus yielding
the best possible performance (assuming the programmer knows what he’s doing)
and traversing up towards the high-level end of the scale languages gain
progressively more expressive power which results in improved programmer
productivity (more “impact” per lines of code), albeit at the cost of runtime
speed. Programmer and program efficiency are widely believed to be mutually
exclusive goals—you just can’t achieve both within the confines of a single
language.&lt;/p&gt;

&lt;p&gt;One solution to this problem has traditionally been the two-language approach,
which boils down to combining a highly expressive but slow high-level language
with a speedy but relatively primitive low-level one as an attempt to gain the
best benefits of both worlds. There are generally two ways to go about this:
by &lt;strong&gt;extending&lt;/strong&gt; or &lt;strong&gt;embedding&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;extending&quot;&gt;Extending&lt;/h3&gt;

&lt;p&gt;When extending, the bulk of the program is written in a high-level language
(either interpreted or statically compiled) to get the benefit of higher
programmer productivity, then performance critical parts simply get rewritten
in a more efficient low-level language. Thus the high-level
language gets &lt;strong&gt;extended&lt;/strong&gt; with special purpose modules or libraries that
handle the performance critical duties.&lt;/p&gt;

&lt;p&gt;Certain languages, such as Python, are more suitable for extending than
embedding. Arguably, one of the best mainstream examples for extending Python
is the hugely popular sci-fi space MMORPG game &lt;a href=&quot;https://en.wikipedia.org/wiki/Eve_Online&quot;&gt;EVE
Online&lt;/a&gt;. The EVE client was written
in &lt;a href=&quot;http://www.grant-olson.net/files/why_stackless.html&quot;&gt;Stackless Python&lt;/a&gt;
that is extended with custom coded C++ modules to handle platform-specific
graphics, sound, network I/O and other performance-sensitive tasks.&lt;/p&gt;

&lt;p&gt;This method might work fine from a performance optimization point of view, but
it’s not without drawbacks. First of all, there is very likely going to be an
“impedance mismatch” between the two languages, which will necessitate to
devise some sort of bridging mechanism to allow the interchange of data
structures and bi-directional control flow between the two layers. This can
quickly result in lots of glue code and data duplication, especially if the
two languages were not designed to co-operate with each other. Also, the
high-level language might have some quite rigid ideas about how the data
should be laid out in memory that might further complicate the low-level
performance optimization efforts (e.g. misaligned data, difficulty of
achieving &lt;a href=&quot;https://deplinenoise.wordpress.com/2013/12/28/optimizable-code/&quot;&gt;SoA
layouts&lt;/a&gt;
(&lt;a href=&quot;https://software.intel.com/en-us/articles/memory-layout-transformations&quot;&gt;Struct of
Arrays&lt;/a&gt;)
for SIMD optimizations etc.) While it’s certainly possible to speed
the program up this way, the time gained by using a high-level language can
easily be amortised by having to build a custom (and potentially suboptimal)
bridging solution and the increased general complexity of the development
workflow.&lt;/p&gt;

&lt;h3 id=&quot;embedding&quot;&gt;Embedding&lt;/h3&gt;

&lt;p&gt;The second variant is writing a high-performance core in a low-level language
(usually a statically compiled one, such as C or C++) and then &lt;strong&gt;embedding&lt;/strong&gt;
a high-level language into the program (typically an interpreted dynamic
scripting language). The core functionality would be exposed through an API
and the application logic would be implemented in the scripting language using
this API. In this scenario, the high-level scripting language is relegated to
an orchestrator role. Another related but slightly different usage pattern for
embedding is to enable the end-user to extend the application via scripts and
plugins, or in the case of games, to allow modding.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.lua.org/&quot;&gt;Lua&lt;/a&gt; is very well suited for embedding; over the years,
it has almost become the de-facto scripting language of choice for C/C++
&lt;a href=&quot;https://en.wikipedia.org/wiki/Lua_(programming_language)#Applications&quot;&gt;applications&lt;/a&gt;,
especially since the advent of &lt;a href=&quot;http://luajit.org/&quot;&gt;LuaJIT&lt;/a&gt;. Due to its very
small memory footprint and high runtime efficiency, it has gained
a &lt;a href=&quot;https://en.wikipedia.org/wiki/Category:Lua-scripted_video_games&quot;&gt;widespread
adoption&lt;/a&gt; in
the C++ dominated gaming industry over the last two decades, and it has
found its way into a wide variety of applications and &lt;a href=&quot;http://www.eluaproject.net/&quot;&gt;embedded
systems&lt;/a&gt; as well, such as high-performance &lt;a href=&quot;http://nginx.org/&quot;&gt;web
servers&lt;/a&gt;, &lt;a href=&quot;http://redis.io/&quot;&gt;in-memory databases&lt;/a&gt;,
&lt;a href=&quot;https://openwrt.org/&quot;&gt;routers&lt;/a&gt; and &lt;a href=&quot;https://www.renoise.com/&quot;&gt;audio
software&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On the desktop application front, the best high-profile commercial example is
undeniably &lt;a href=&quot;https://en.wikipedia.org/wiki/Adobe_Photoshop_Lightroom&quot;&gt;Adobe Photoshop
Lightroom&lt;/a&gt;.  More
than 60% of the Lightroom code was &lt;a href=&quot;http://www.troygaul.com/LrExposedC4.html&quot;&gt;written in
Lua&lt;/a&gt; (basically the whole UI logic),
C++ was only used for the speed-critical image-manipulation routines and some
platform-specific glue code.&lt;/p&gt;

&lt;p&gt;Of course, the downside of the embedding approach is that a significant part
of the program still has to be written in C/C++, which leads right into my
next topic…&lt;/p&gt;

&lt;h2 id=&quot;life-is-too-short-for-c&quot;&gt;Life is too short for C++&lt;/h2&gt;

&lt;p&gt;I can almost hear some people murmuring in the back row at this point: &lt;em&gt;“But
we already have a language that combines ultimate bare-metal performance
with high-level productivity: it’s called C++!”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Um, no. That couldn’t be further from the truth.  For a start, read the
preceding section again—if C++ was so awesome at everything, why would
people want to embed high-level scripting languages into their applications?
(Apart from plugin support and giving gameplay scripters a much simpler
language to use, which are some very good reasons, but let’s focus on
developer productivity here.)&lt;/p&gt;

&lt;p&gt;C++ is a &lt;em&gt;total disaster&lt;/em&gt; of a language, a bloated and overcomplicated
monstrosity. Its only redeeming quality is its (almost) full
backwards-compatibilty with C. That may sound overly harsh, but look at a few
successful C++
&lt;a href=&quot;http://wiki.scummvm.org/index.php/Coding_Conventions&quot;&gt;open&lt;/a&gt;-&lt;a href=&quot;https://github.com/tonioni/WinUAE/&quot;&gt;source&lt;/a&gt;
&lt;a href=&quot;https://sourceforge.net/projects/dosbox&quot;&gt;projects&lt;/a&gt; and check out some
&lt;a href=&quot;http://c0de517e.blogspot.com/2011/02/surviving-c.html&quot;&gt;blog&lt;/a&gt; &lt;a href=&quot;http://voodoo-slide.blogspot.com/2010/01/amplifying-c.html&quot;&gt;posts&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/ID_AA_Carmack/status/452586892282695680&quot;&gt;rants&lt;/a&gt; written by AAA game developers who are forced to
use this abomination on a daily basis. You’ll quickly see a common pattern
emerging: the general best practice is to just stick to a very narrow,
restricted subset of the language and stay away from its so-called “high-level
abstractions”—the existence of which should normally be &lt;em&gt;the&lt;/em&gt; main reason
for choosing it over C in the first place.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-03-03/hydra_by_ruth_tay.jpg&quot; alt=&quot;A handsome Greek hydra&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot; width: 100%;&quot;&gt;Contrary to popular belief, C++ was well-known and feared among the ancient Greeks. The hellish abomination was commonly depicted as a fiendish, many-headed water serpent rising from the dark depths of the sea, each head representing a different programming paradigm totally incompatible with the rest of the language. The actual number of fully functional heads varies greatly and depends on the target platform, the version of the C++ standard being depicted, the compiler vendor, the exact shade of the compiler vendor&#39;s CEO&#39;s wife&#39;s niece&#39;s toe polish, and the favourite food of the chief compiler implementor&#39;s pet baby wombat. (Illustration by &lt;a href=&quot;http://ruthtaylor.nl&quot;&gt;Ruth Taylor&lt;/a&gt;)&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;So what happens with C++ on a successful project? The wretched standard
library flies &lt;a href=&quot;http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2271.html&quot;&gt;right out of the
window&lt;/a&gt;
(along with other ghoulish aberrations originating from the deepest bowels of
hell like
&lt;a href=&quot;https://twitter.com/ID_AA_Carmack/status/81104943490146304&quot;&gt;Boost&lt;/a&gt;), operator
overloading is basically &lt;em&gt;streng verboten&lt;/em&gt; at the risk of public corporal punishment
(except for the simplest cases), template metaprogramming is best left untouched
(but only if you value your sanity and want to get a working build out of
the miserable compiler on the same day), so in the end what you’re left with
is pretty much plain old C with some extra bells-and-whistles tacked on its
back.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;C++: an octopus made by nailing extra legs onto a dog.&lt;/p&gt;
  &lt;footer&gt;&amp;mdash; &lt;cite&gt;Steve Taylor&lt;/cite&gt;&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;p&gt;And don’t even get me started with C++11—it’s a lesser known that what the
“11” actually signifies in that name is the number of extra tentacles bolted
to the language in an attempt to make it suck less hard…&lt;/p&gt;

&lt;p&gt;So it’s a fair conclusion to say that using C++ in a sane way is &lt;a href=&quot;http://harmful.cat-v.org/software/c++/linus&quot;&gt;using it as
a C&lt;/a&gt;.  Let’s remember that C is
ultimately nothing more than a clever cross-platform assembler (plus
a somewhat usable standard library). I do like C, it’s a nice minimal
system-programming language that’s almost perfect at what it aims to do, and
I have no problems using it for &lt;em&gt;low-level work&lt;/em&gt;.  But using it as
a high-level language is just a futile exercise in frustration.&lt;/p&gt;

&lt;p&gt;(Let me also quickly add that I have the &lt;em&gt;utmost respect&lt;/em&gt; for all C++ coders
out there who are capable of producing useful software with this beast of
a language, despite all the odds (e.g. most game developers). Just to set the
record straight, I am criticising the &lt;em&gt;language&lt;/em&gt;, not its &lt;em&gt;users&lt;/em&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;um-is-this-the-best-we-got&quot;&gt;Um, is this the best we got?&lt;/h2&gt;

&lt;p&gt;Ok, back on track after this slight C++ bashing detour. The thing is, as long
as C++ is still &lt;a href=&quot;http://pypl.github.io/PYPL.html&quot;&gt;alive&lt;/a&gt; and
&lt;a href=&quot;http://www.tiobe.com/index.php/tiobe_index&quot;&gt;well&lt;/a&gt;, I think it rightfully
deserves all the bashing it can get. And if you really love it,
congratulations, you’re one happy programmer, a content user of one of the
most popular languages in existence on this planet today! That’s awesome,
I wish you all the best, have fun and fare well!&lt;/p&gt;

&lt;p&gt;Meanwhile, things are not looking exactly spectacular on our side. It seems
that there’s nothing we can possibly do about this rather unfortunate
situation if we want both ultimate programmer productivity and the best
runtime efficiency, right?  This is just the way it is, so we’d better suck it
up and prepare to mount ourselves onto the back of our strange two-language
mule. Being the astute reader of this fine piece of publication that you
certainly are, you must have no doubt figured it out by now that there &lt;em&gt;must
be a better way…&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;redefining-the-goal&quot;&gt;Redefining the GOAL&lt;/h2&gt;

&lt;p&gt;One day I came across an &lt;a href=&quot;https://news.ycombinator.com/item?id=2090554&quot;&gt;interesting
post&lt;/a&gt; on HackerNews about a most
unusual language called
&lt;a href=&quot;https://en.wikipedia.org/wiki/Game_Oriented_Assembly_Lisp&quot;&gt;GOAL&lt;/a&gt; that the
game studio &lt;a href=&quot;https://en.wikipedia.org/wiki/Naughty_Dog&quot;&gt;Naughty Dog&lt;/a&gt; created
for the development of their older PlayStation titles. Now, &lt;strong&gt;GOAL&lt;/strong&gt; stands
for &lt;strong&gt;Game Oriented Assembly Lisp&lt;/strong&gt;, and as the name implies, it allows the
programmer to &lt;em&gt;seamlessly intermix&lt;/em&gt; high-level Lisp code with
down-to-the-metal assembly in the same lexical environment.  Of course, it
also supported all the killer features commonly associated with Lisp, such as
inspecting and changing the program while it’s running, macros and all
that sort of stuff (check out the &lt;a href=&quot;http://web.archive.org/web/20070127022728/http://lists.midnightryder.com/pipermail/sweng-gamedev-midnightryder.com/2005-August/003804.html&quot;&gt;code
example&lt;/a&gt;
in this forum post by one of the Naughty Dog devs).&lt;/p&gt;

&lt;p&gt;I found this idea extremely cool! A language that integrates the two opposite
extremes of the abstraction spectrum into a single coherent form! Assembly
code would be written as S-expressions, so it could be generated using macros
or manipulated as data just like Lisp code. No more messing around with
language bridges, it’s all a single package.&lt;/p&gt;

&lt;p&gt;The concept just got stuck in my head and I sort of got obsessed with the
topic. I had been trying to seek out all information I could on GOAL, but
sadly there was not too much to be found on the internet.  The original
sources are now owned by Sony, so it’s a safe bet that we’ll never get
a chance to take a peek into them.  Luckily, &lt;a href=&quot;http://all-things-andy-gavin.com/about/&quot;&gt;Andy
Gavin&lt;/a&gt;, the genius who created GOAL
had written up a &lt;a href=&quot;http://all-things-andy-gavin.com/2011/03/12/making-crash-bandicoot-gool-part-9/&quot;&gt;quite nice
overview&lt;/a&gt;
on it which is definitely worth reading (for more interesting reading
material, check out the reading suggestions at the end of this article).&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  
      &lt;img src=&quot;/files/2016-03-03/goal.jpg&quot; alt=&quot;GOAL source code&quot; style=&quot;width: 100%&quot; /&gt;
  

  
    
    &lt;figcaption style=&quot;&quot;&gt;After the Sony acquisition, Naughty Dog was forced to abandon Lisp in favour of C++.&lt;br /&gt;Question: What will become of Lisp code after Sony has bought your company? Answer: A block comment. (From the video &lt;a href=&quot;https://www.youtube.com/watch?v=LHvUjmlWRAI&quot;&gt;The Making of Jak &amp;amp; Daxter&lt;/a&gt;)&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;It is also important to note that  GOAL was not just simply a language but
a whole interactive system that allowed rapid prototyping of ideas and
exploratory programming, as described in &lt;a href=&quot;http://art-of-optimization.blogspot.ca/2014/06/the-legacy-of-goal.html&quot;&gt;this
post&lt;/a&gt;
by a veteran Naughty Dog developer.&lt;/p&gt;

&lt;p&gt;Well, this all sounds very much what I’d like my ideal programming language to
be: interactive and highly-expressive while not sacrificing runtime
efficiency. So the question is, does such a language exist somewhere today,
silently waiting to be discovered? We’ll find that out in the next part!&lt;/p&gt;

&lt;hr /&gt;

&lt;section class=&quot;links&quot;&gt;

  &lt;h2 id=&quot;suggested-reading&quot;&gt;Suggested reading&lt;/h2&gt;

  &lt;h3 id=&quot;the-good&quot;&gt;The Good&lt;/h3&gt;

  &lt;ul class=&quot;compact&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;http://all-things-andy-gavin.com/2011/03/12/making-crash-bandicoot-gool-part-9/&quot;&gt;Making Crash Bandicoot – GOOL – part 9&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://art-of-optimization.blogspot.ca/2014/06/the-legacy-of-goal.html&quot;&gt;The Legacy of GOAL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://all-things-andy-gavin.com/goal-test/&quot;&gt;Goal Test&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://all-things-andy-gavin.com/2011/10/25/lispings-ala-john-mccarthy/&quot;&gt;Lispings ala John McCarthy&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://franz.com/success/customer_apps/animation_graphics/naughtydog.lhtml&quot;&gt;Raising the Paradigm of Video Gaming – Again…Thanks to Lisp and Allegro CL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.gamasutra.com/view/feature/131394/postmortem_naughty_dogs_jak_and_.php&quot;&gt;Postmortem: Naughty Dog’s Jak and Daxter: the Precursor Legacy&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

  &lt;h3 id=&quot;the-bad&quot;&gt;The Bad&lt;/h3&gt;

  &lt;ul class=&quot;compact&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;http://c0de517e.blogspot.com/2011/02/surviving-c.html&quot;&gt;Surviving C++&lt;/a&gt; (the comments are just as educational as the article)&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://c0de517e.blogspot.com/2014/06/where-is-my-c-replacement.html&quot;&gt;Where is my C++ replacement?&lt;/a&gt; (ditto)&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://yosefk.com/c++fqa/&quot;&gt;C++ FQA Lite — Defective C++&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

  &lt;h3 id=&quot;the-ugly&quot;&gt;The Ugly&lt;/h3&gt;

  &lt;ul class=&quot;compact&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;https://gigamonkeys.wordpress.com/2009/10/16/coders-c-plus-plus/&quot;&gt;C++ in Coders at Work&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://harmful.cat-v.org/software/c++/linus&quot;&gt;Linus Torvalds on C++&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://harmful.cat-v.org/software/c++/&quot;&gt;C++ is Good for the Economy, It Creates Jobs!&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

&lt;/section&gt;
</description>
                <pubDate>Thu, 03 Mar 2016 00:00:00 +1000</pubDate>
                <link>http://blog.johnnovak.net/2016/03/03/the-quest-for-a-programming-language-that-doesnt-suck-part1/</link>
                <guid isPermaLink="true">http://blog.johnnovak.net/2016/03/03/the-quest-for-a-programming-language-that-doesnt-suck-part1/</guid>
            </item>
        
            <item>
                <title>The Beginning</title>
                <description>&lt;p&gt;Just what the world needs, yet another technical blog!&lt;/p&gt;

&lt;p&gt;My name is &lt;strong&gt;John Novak&lt;/strong&gt; and I’ve been working as a professional programmer
ever since I left uni in 2003. In those last 12+ years I have worked in
a variety of industries using lots of different programming languages, most
notably C, C++, Java, Python, C#, JavaScript and PHP. I’ll spare you from the
gory details, you can check out my &lt;a href=&quot;https://www.linkedin.com/in/johnnovak1979&quot;&gt;LinkedIn profile&lt;/a&gt; if you’re really
that interested.&lt;/p&gt;

&lt;p&gt;During my teenager years I was fairly involved with the Amiga and PC
&lt;a href=&quot;https://en.wikipedia.org/wiki/Demoscene&quot;&gt;demoscene&lt;/a&gt; as a graphician and musician. That was also the time when
I taught myself coding in Pascal, x86 assembly and C as a hobby (in that exact
order). I still write &lt;a href=&quot;http://www.johnnovak.net/#music&quot;&gt;music&lt;/a&gt; in my spare time and do
&lt;a href=&quot;http://photo.johnnovak.net/&quot;&gt;photography&lt;/a&gt;, plus occasionally dabble in design (such as the visuals
of this blog).&lt;/p&gt;

&lt;p&gt;In the last 5+ years, my programming jobs involved me to focus quite heavily
on the server-side development side of things, mostly in Java and now recently
in Scala. While that’s a very interesting and rewarding area to work in with
a lot of depth to it, I’ve come to realise that it’s just too easy to fall
into a comfortable routine and become an expert in an area while
simultaneously missing out on all the exciting new stuff that’s happening in
the world of programming outside your particular niche.&lt;/p&gt;

&lt;p&gt;And that’s the main motivation for starting this blog—to keep myself up to
date on new technologies that interest me and document my tinkerings. The idea
is that using the blog format to report on my experiments will help me focus
and also make me more diligent in researching my topics.  After all, the goal
is to write about things that are new to me, so trying to keep the amount of
misinformation I’m unwillingly (but inevitably) going to spread to a minimum
is probably a good idea, as I’m sure the experts will be far too eager to
chime in and correct me… :)&lt;/p&gt;

&lt;p&gt;Having said that, don’t hesitate to point out any of my factual mistakes in
the comments. In fact, I’d be glad if you did and I encourage you to do so
because I’m here to learn. That offer stands for correcting my grammar as well
as English is not my first language.&lt;/p&gt;

&lt;p&gt;Alright, enough of rambling about myself already, see you in the next post!&lt;/p&gt;

</description>
                <pubDate>Sun, 21 Feb 2016 00:00:00 +1000</pubDate>
                <link>http://blog.johnnovak.net/2016/02/21/the-beginning/</link>
                <guid isPermaLink="true">http://blog.johnnovak.net/2016/02/21/the-beginning/</guid>
            </item>
        
    </channel>
</rss>

